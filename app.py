import streamlit as st
import pandas as pd
import time
import os
import re
import ast 
from concurrent.futures import ThreadPoolExecutor, as_completed

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from modules.parsers import parse_references_with_anystyle
# å°å…¥æœ¬åœ°è³‡æ–™åº«æ¨¡çµ„
from modules.local_db import load_csv_data, search_local_database
# å°å…¥ API å®¢æˆ¶ç«¯
from modules.api_clients import (
    get_scopus_key,
    get_serpapi_key,
    search_crossref_by_doi,
    search_crossref_by_text,
    search_scopus_by_title,
    search_scholar_by_title,
    search_scholar_by_ref_text,
    search_s2_by_title,
    search_openalex_by_title,
    check_url_availability
)

# ========== é é¢è¨­å®š ==========
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (ç³»çµ±å¢å¼·ç‰ˆ)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; margin-right: 5px; }
    .ref-box { background-color: #f8f9fa; padding: 10px; border-radius: 5px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    
    div[data-testid="stMarkdownContainer"] table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 10px;
    }
    div[data-testid="stMarkdownContainer"] td {
        padding: 8px 5px;
        border-bottom: 1px solid #f0f0f0;
        font-size: 0.95em;
    }
    div[data-testid="stMarkdownContainer"] th {
        display: none; 
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (è§£æè£œæ•‘å¢å¼·ç‰ˆ)</div>', unsafe_allow_html=True)

# ========== Session State ==========
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ========== [è¼”åŠ©] 1. äººåæ ¼å¼åŒ– ==========
def format_name_field(data):
    if not data: return None
    if isinstance(data, str) and not (data.startswith('[') or data.startswith('{')): return data
    try:
        if isinstance(data, str):
            try: data = ast.literal_eval(data)
            except: return data
        names_list = []
        if isinstance(data, dict): data = [data]
        elif not isinstance(data, list): return str(data)
        for item in data:
            if isinstance(item, dict):
                parts = []
                if item.get('family'): parts.append(item['family'])
                if item.get('given'): parts.append(item['given'])
                if parts: names_list.append(", ".join(parts))
            else:
                names_list.append(str(item))
        return "; ".join(names_list)
    except:
        return str(data)

# ========== [æ ¸å¿ƒè£œæ•‘] 2. è³‡æ–™æ¸…æ´—èˆ‡æ¨™é¡Œæå–ä¿®æ­£ ==========
def refine_parsed_data(parsed_item):
    item = parsed_item.copy()
    raw_text = item.get('text', '').strip()

    # åŸºç¤ç¬¦è™Ÿæ¸…æ´—
    for key in ['doi', 'url', 'title', 'date']:
        if item.get(key) and isinstance(item[key], str):
            item[key] = item[key].strip(' ,.;)]}>')

    # --- [æ ¸å¿ƒè£œæ•‘ï¼šè™•ç† StyleTTS 2 / AIOS ç­‰æ ¼å¼] ---
    title = item.get('title', '')
    
    # å¦‚æœæ¨™é¡Œæ²’æŠ“åˆ°ï¼Œæˆ–æ˜¯è¢«èª¤åˆ¤ç‚ºç¸®å¯« (é•·åº¦å¤ªçŸ­)
    if not title or len(title) < 10:
        # æ¨¡å¼ A: é‡å° "ç¸®å¯«: å®Œæ•´æ¨™é¡Œ" (å¦‚ AIOS: LLM Agent...)
        # åŒ¹é…é–‹é ­æ˜¯ 2-12 å€‹å­—å…ƒï¼Œç·Šæ¥å†’è™Ÿï¼ŒæŠ“å–åˆ°ä¸‹ä¸€å€‹åˆ†éš”ç¬¦
        abbr_match = re.search(r'^([A-Z0-9\-\.\s]{2,12}:\s*.+?)(?=\s*[,\[]|\s*Available|\s*\(|\bhttps?://|\.|$)', raw_text)
        if abbr_match:
            item['title'] = abbr_match.group(1).strip()
        else:
            # æ¨¡å¼ B: AnyStyle æŠŠæ¨™é¡Œèª¤åˆ¤ç‚ºå‡ºç‰ˆå•†æˆ–æœŸåˆŠ
            for backup_key in ['publisher', 'container-title', 'journal']:
                val = item.get(backup_key)
                if val and len(str(val)) > 15:
                    item['title'] = str(val).strip()
                    break

    # DOI æå–é‚è¼¯
    url_val = item.get('url', '')
    if url_val:
        doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', url_val)
        if doi_match:
            item['doi'] = doi_match.group(1).strip('.')

    # è™•ç†äººå
    if item.get('authors'): item['authors'] = format_name_field(item['authors'])
    if item.get('editor'): item['editor'] = format_name_field(item['editor'])
    
    return item

# ========== å´é‚Šæ¬„ ==========
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥æœ¬åœ°åº«: {len(local_df)} ç­†")
            target_col = "è«–æ–‡åç¨±" if "è«–æ–‡åç¨±" in local_df.columns else local_df.columns[0]
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"Scopus API: {'âœ…' if scopus_key else 'âŒ'}")
    st.info(f"SerpAPI: {'âœ…' if serpapi_key else 'âŒ'}")

# ========== ä¸»é é¢ ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    raw_input = st.text_area("åœ¨æ­¤è¼¸å…¥æ–‡ç»å…§å®¹...", height=300, placeholder="ä¾‹å¦‚: StyleTTS 2: Towards Human-Level Text-to-Speech...")
    
    if st.button("ğŸš€ é–‹å§‹è§£æ", type="primary"):
        if not raw_input:
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            with st.spinner("AnyStyle è§£æä¸­..."):
                _, struct_list = parse_references_with_anystyle(raw_input)
            if struct_list:
                st.session_state.structured_references = struct_list
                st.success(f"âœ… è§£ææˆåŠŸï¼å…± {len(struct_list)} ç­†ã€‚")
            else:
                st.error("âŒ AnyStyle è§£æç•°å¸¸ã€‚")

with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹å…¨è‡ªå‹•é©—è­‰ (ä½µç™¼æ¨¡å¼)", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_task(idx, raw_ref):
                # 1. è§£æè£œæ•‘
                ref = refine_parsed_data(raw_ref)
                
                # 2. æº–å‚™æœç´¢å­—ä¸² (æ ¸å¿ƒä¿åº•)
                title = ref.get('title', '')
                text = ref.get('text', '')
                # å¦‚æœæ¨™é¡Œä¸å­˜åœ¨æˆ–å¤ªçŸ­ï¼Œç›´æ¥ç”¨æ•´è¡ŒåŸå§‹æ–‡å­—çš„å‰ 120 å­—å»æœç´¢
                search_query = title if (title and len(title) > 8) else text[:120]
                
                doi = ref.get('doi')
                parsed_url = ref.get('url')
                first_author = ref['authors'].split(';')[0].split(',')[0].strip() if ref.get('authors') else ""

                res = {
                    "id": idx, "title": title if title else "è§£æå¤±æ•— (ä½¿ç”¨ä¿åº•æœç´¢)", 
                    "text": text, "parsed": ref,
                    "sources": {}, "found_at_step": None, "debug_logs": {} 
                }

                # Step 0: Local DB (é‡å°ä¸­æ–‡)
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', search_query))
                if has_chinese and local_df is not None and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref (DOI)
                if doi:
                    _, url, status = search_crossref_by_doi(doi, target_title=title if title else None)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref (DOI)"
                        return res
                
                # Step 2: Crossref (Text Search)
                url, status = search_crossref_by_text(search_query, first_author)
                if url:
                    res["sources"]["Crossref"] = url
                    res["found_at_step"] = "1. Crossref (Search)"
                    return res

                # Step 3: Scopus
                if scopus_key:
                    url, status = search_scopus_by_title(search_query, scopus_key)
                    if url:
                        res["sources"]["Scopus"] = url
                        res["found_at_step"] = "2. Scopus"
                        return res

                # Step 4: OpenAlex / S2 / Scholar
                for api_func, step_name in [
                    (lambda: search_openalex_by_title(search_query, first_author), "3. OpenAlex"),
                    (lambda: search_s2_by_title(search_query, first_author), "4. Semantic Scholar"),
                    (lambda: search_scholar_by_title(search_query, serpapi_key), "5. Google Scholar")
                ]:
                    try:
                        url, status = api_func()
                        if url:
                            res["sources"][step_name.split(". ")[1]] = url
                            res["found_at_step"] = step_name
                            return res
                        res["debug_logs"]["Scholar (Title)"] = status
                    
                    # â–¼â–¼â–¼â–¼â–¼ ä¿®æ”¹é–‹å§‹ â–¼â–¼â–¼â–¼â–¼
                    # åŸæœ¬çš„è£œæ•‘æœå°‹é‚è¼¯ (èˆŠç‰ˆæœƒç›´æ¥ return resï¼Œç¾åœ¨æ”¹æ‰)
                    url_r, status_r = search_scholar_by_ref_text(text, serpapi_key, target_title=title)
                    if url_r:
                        # [è®Šæ›´é»] ä¸å†è¦–ç‚º "sources" (é©—è­‰æˆåŠŸ)ï¼Œè€Œæ˜¯å­˜å…¥ "suggestion"
                        res["suggestion"] = url_r
                        res["debug_logs"]["Scholar (Suggestion)"] = "æ‰¾åˆ°ç›¸ä¼¼çµæœï¼Œä½†å› è¼¸å…¥æœ‰èª¤æœªåˆ—å…¥é©—è­‰æˆåŠŸ"
                        # [é‡è¦] é€™è£¡ç§»é™¤äº† return resï¼Œè®“ç¨‹å¼ç¹¼çºŒå¾€ä¸‹è·‘
                        # é€™æ¨£å¦‚æœå¾Œé¢ Step 6 ç¶²ç«™æª¢æŸ¥ä¹Ÿæ²’éï¼Œæœ€çµ‚ç‹€æ…‹å°±æœƒæ˜¯ "âŒ æœªæ‰¾åˆ°"
                    else:
                        res["debug_logs"]["Scholar (Text)"] = status_r

                # Step 5: Website URL Check
                if parsed_url and parsed_url.startswith('http'):
                    if check_url_availability(parsed_url):
                        res["sources"]["Direct Link"] = parsed_url
                        res["found_at_step"] = "6. Website / Direct URL"
                    else:
                        res["sources"]["Direct Link (Dead)"] = parsed_url
                        res["found_at_step"] = "6. Website (Link Failed)"
                
                return res

            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(check_single_task, i+1, r): i for i, r in enumerate(refs)}
                for i, future in enumerate(as_completed(futures)):
                    results_buffer.append(future.result())
                    progress.progress((i + 1) / total)
                    status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            st.rerun()

    # --- çµæœå±•ç¤ºå€ ---
    if st.session_state.results:
        st.divider()
        col_f1, col_f2 = st.columns([1, 2])
        with col_f1:
            filter_option = st.selectbox("ğŸ“‚ ç¯©é¸çµæœ", ["å…¨éƒ¨é¡¯ç¤º", "âœ… å·²é©—è­‰", "âŒ æœªæ‰¾åˆ°"])
        
        for res in st.session_state.results:
            found_step = res.get('found_at_step')
            is_found = found_step is not None
            
            if filter_option == "âœ… å·²é©—è­‰" and not is_found: continue
            if filter_option == "âŒ æœªæ‰¾åˆ°" and is_found: continue

            bg_color = "#D1FAE5" if is_found else "#FEE2E2"
            p = res.get('parsed', {})
            
            with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}"):
                st.markdown(f'<div style="background:{bg_color}; padding:10px; border-radius:5px;"><b>é©—è­‰ç‹€æ…‹:</b> {found_step if is_found else "æœªæ‰¾åˆ°åŒ¹é…"}</div>', unsafe_allow_html=True)
                
                st.markdown(f"""
                <div style="padding-top: 10px;">
                    <span class="status-badge" style="background:#D1FAE5; color:#065F46;">ğŸ“š è³‡æ–™åº«: {verified_db_count}</span>
                    <span class="status-badge" style="background:#DBEAFE; color:#1E40AF;">ğŸŒ æœ‰æ•ˆç¶²ç«™: {valid_web_count}</span>
                    <span class="status-badge" style="background:#FEF3C7; color:#92400E;">âš ï¸ ç¶²ç«™(Fail): {failed_web_count}</span>
                    <span class="status-badge" style="background:#FEE2E2; color:#991B1B;">âŒ æœªæ‰¾åˆ°: {unverified_count}</span>
                </div>
                """, unsafe_allow_html=True)

            st.divider()

            for res in st.session_state.results:
                found_step = res.get('found_at_step')
                is_db_verified = found_step and "Website" not in found_step
                is_web_valid = found_step == "6. Website / Direct URL"
                is_web_failed = found_step == "6. Website (Link Failed)"
                
                if filter_option == "âœ… è³‡æ–™åº«é©—è­‰" and not is_db_verified: continue
                if filter_option == "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº" and not is_web_valid: continue
                if filter_option == "âš ï¸ ç¶²ç«™ (é€£ç·šå¤±æ•—)" and not is_web_failed: continue
                if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and (is_db_verified or is_web_valid or is_web_failed): continue

                bg_color = "#FEE2E2"
                if is_db_verified: bg_color = "#D1FAE5"
                elif is_web_valid: bg_color = "#DBEAFE"
                elif is_web_failed: bg_color = "#FEF3C7"
                
                status_label = f"âœ… {found_step}" if is_db_verified else (f"ğŸŒ {found_step}" if is_web_valid else (f"âš ï¸ {found_step}" if is_web_failed else "âŒ æœªæ‰¾åˆ°"))
                
                p = res.get('parsed', {})
                with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                    st.markdown(f"""<div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-bottom: 15px;"><b>ç‹€æ…‹:</b> {status_label}</div>""", unsafe_allow_html=True)
                    
                    display_author = p.get('authors') or (f"{p['editor']} (Ed.)" if p.get('editor') else "N/A")
                    display_title = p.get('title', 'N/A') + (f" {p['edition']}" if p.get('edition') else "")
                    source_parts = [x for x in [p.get('container-title'), p.get('journal'), f"{p.get('location')}: {p.get('publisher')}" if p.get('publisher') else p.get('publisher')] if x]
                    display_source = ", ".join(source_parts) if source_parts else "N/A"
                    
                    st.markdown(f"""
                    | | |
                    | :--- | :--- |
                    | **ğŸ‘¥ ä½œè€…/ç·¨è€…** | `{display_author}` |
                    | **ğŸ“… ç™¼è¡¨å¹´ä»½** | `{p.get('date', 'N/A')}` |
                    | **ğŸ“° æ–‡ç»æ¨™é¡Œ** | `{display_title}` |
                    | **ğŸ¢ å‡ºè™•/ç™¼è¡Œ** | `{display_source}` |
                    | **ğŸ”¢ DOI/URL** | `{p.get('doi', p.get('url', 'N/A'))}` |
                    """)
                    st.divider()
                    st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                    st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                    
                    # â–¼â–¼â–¼â–¼â–¼ æ–°å¢é€™æ®µç¨‹å¼ç¢¼ â–¼â–¼â–¼â–¼â–¼
                    if res.get("suggestion"):
                        st.warning("ğŸ’¡ **è¼¸å…¥å¯èƒ½æœ‰èª¤ï¼Œç³»çµ±å»ºè­°ï¼š**")
                        st.markdown(f"ç³»çµ±åœ¨æ¨¡ç³Šæœå°‹ä¸­æ‰¾åˆ°äº†ç›¸ä¼¼æ–‡ç»ï¼Œè«‹ç¢ºèªæ‚¨æ˜¯å¦æ˜¯æŒ‡ï¼š\n\nğŸ‘‰ **[é»æ“ŠæŸ¥çœ‹ Google Scholar å»ºè­°çµæœ]({res['suggestion']})**")
                        st.caption("æ³¨æ„ï¼šæ­¤æ–‡ç»å› åŸå§‹è¼¸å…¥æ¨™é¡Œ/æ ¼å¼ä¸ç²¾ç¢ºï¼Œæœªè¢«æ¨™è¨˜ç‚ºã€Œé©—è­‰æˆåŠŸã€ã€‚")
                        st.divider() # åŠ å€‹åˆ†éš”ç·šç¾è§€ä¸€é»
                    # â–²â–²â–²â–²â–² æ–°å¢çµæŸ â–²â–²â–²â–²â–²
                    
                    if res['sources']:
                        st.write("**ğŸ”— é©—è­‰ä¾†æºé€£çµï¼š**")
                        for src, link in res['sources'].items():
                            if src == "Direct Link": st.markdown(f"- ğŸŒ **åŸå§‹ç¶²ç«™ (å·²æ¸¬è©¦å¯é€£ç·š)**: [é»æ“Šå‰å¾€]({link})")
                            elif src == "Direct Link (Dead)": st.markdown(f"- âš ï¸ **åŸå§‹ç¶²ç«™ (é€£ç·šé€¾æ™‚/å¤±æ•—ï¼Œè«‹æ‰‹å‹•ç¢ºèª)**: [é»æ“Šå‰å¾€]({link})")
                            elif link.startswith("http"): st.markdown(f"- **{src}**: [é»æ“Šé–‹å•Ÿ]({link})")
                            else: st.markdown(f"- **{src}**: {link}")
                    else:
                        st.error("âš ï¸ åœ¨æ‰€æœ‰å•Ÿç”¨çš„è³‡æ–™åº«ä¸­çš†æœªæ‰¾åˆ°åŒ¹é…é …ã€‚")
                        with st.expander("ğŸ” æŸ¥çœ‹æ¯å€‹è³‡æ–™åº«çš„è©³ç´°å¤±æ•—åŸå›  (Debug Logs)"):
                            if res.get("debug_logs"):
                                for api, msg in res["debug_logs"].items():
                                    st.write(f"**{api}**: {msg}")
                            else:
                                st.write("æ²’æœ‰å¯ç”¨çš„è¨ºæ–·è¨˜éŒ„ã€‚")

with tab3:
    if st.session_state.results:
        df_res = pd.DataFrame(st.session_state.results)
        st.metric("ç¸½æŸ¥æ ¸æ•¸", len(df_res))
        st.subheader("é©—è­‰ä¾†æºåˆ†ä½ˆ")
        st.bar_chart(df_res['found_at_step'].fillna('Not Found').value_counts())
        
        csv = df_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š CSV", csv, "report.csv", "text/csv")
    else:
        st.info("å°šç„¡çµ±è¨ˆæ•¸æ“š")