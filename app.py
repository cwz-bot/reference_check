# app.py

import streamlit as st
import pandas as pd
import time
import os
import re
import ast 
from concurrent.futures import ThreadPoolExecutor, as_completed

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from modules.parsers import parse_references_with_anystyle
# å°å…¥æœ¬åœ°è³‡æ–™åº«æ¨¡çµ„
from modules.local_db import load_csv_data, search_local_database
# å°å…¥ API å®¢æˆ¶ç«¯
from modules.api_clients import (
    get_scopus_key,
    get_serpapi_key,
    search_crossref_by_doi,
    search_crossref_by_text,
    search_scopus_by_title,
    search_scholar_by_title,
    search_scholar_by_ref_text,
    search_s2_by_title,
    search_openalex_by_title,
    check_url_availability
)

# ========== é é¢è¨­å®š ==========
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (Local DB + Docker)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; margin-right: 5px; }
    .ref-box { background-color: #f8f9fa; padding: 10px; border-radius: 5px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    
    div[data-testid="stMarkdownContainer"] table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 10px;
    }
    div[data-testid="stMarkdownContainer"] td {
        padding: 8px 5px;
        border-bottom: 1px solid #f0f0f0;
        font-size: 0.95em;
    }
    div[data-testid="stMarkdownContainer"] th {
        display: none; 
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (æ··åˆé›²åœ°ç‰ˆ)</div>', unsafe_allow_html=True)

# ========== Session State ==========
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ========== [è¼”åŠ©] 1. äººåæ ¼å¼åŒ– ==========
def format_name_field(data):
    if not data: return None
    if isinstance(data, str) and not (data.startswith('[') or data.startswith('{')): return data
    try:
        if isinstance(data, str):
            try: data = ast.literal_eval(data)
            except: return data
        names_list = []
        if isinstance(data, dict): data = [data]
        elif not isinstance(data, list): return str(data)
        for item in data:
            if isinstance(item, dict):
                parts = []
                if item.get('family'): parts.append(item['family'])
                if item.get('given'): parts.append(item['given'])
                if parts: names_list.append(", ".join(parts))
            else:
                names_list.append(str(item))
        return "; ".join(names_list)
    except:
        return str(data)

# ========== [æ ¸å¿ƒä¿®æ”¹] 2. è³‡æ–™æ¸…æ´—èˆ‡æ‹†åˆ†ä¿®æ­£ (ç©¶æ¥µç‰ˆ) ==========
def refine_parsed_data(parsed_item):
    """
    ä¿®æ­£ AnyStyle è§£æçµæœï¼ŒåŒ…å«å¼·åŠ› DOI æå–èˆ‡ RFC æ¨™é¡Œæ•‘æ´ã€‚
    """
    item = parsed_item.copy()
    
    # 1. åŸºç¤æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰æ¬„ä½çš„å°¾éƒ¨æ¨™é»
    for key in ['doi', 'url', 'title', 'date']:
        if item.get(key) and isinstance(item[key], str):
            item[key] = item[key].strip(' ,.;)]}>')

    # 2. [DOI å¼·åŠ›æ•‘æ´] 
    # æƒæ URL æ¬„ä½ï¼Œå°‹æ‰¾æ˜¯å¦éš±è—äº† DOI (æ ¼å¼: 10.xxxx/xxxx)
    url_val = item.get('url', '')
    if url_val:
        # Regex è§£é‡‹: åŒ¹é… 10. é–‹é ­ï¼Œæ¥è‘—4-9ä½æ•¸å­—ï¼Œæ–œç·šï¼Œç„¶å¾Œæ˜¯ä»»æ„å­—å…ƒ
        doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', url_val)
        if doi_match:
            extracted_doi = doi_match.group(1).strip('.')
            item['doi'] = extracted_doi
            
            # å¦‚æœ URL åªæ˜¯ DOI çš„é€£çµ (å¦‚ https://doi.org/10...)ï¼Œå‰‡æ¸…ç©º URL
            # é€™æ¨£å¯ä»¥é¿å… Step 6 æŠŠå®ƒç•¶ä½œç¶²ç«™å»æª¢æŸ¥
            if 'doi.org' in url_val or url_val.replace('http://', '').startswith(extracted_doi):
                item['url'] = None
    
    # 3. æ¨™é¡Œæ•‘æ´ (RFC ç­‰ç‰¹æ®Šæ ¼å¼)
    title = item.get('title', '')
    # [æ–°å¢] journal æ¬„ä½ï¼Œå› ç‚ºæœ‰æ™‚å€™ AnyStyle æœƒæŠŠé•·å­—ä¸²å¡åœ¨é€™è£¡
    garbage_fields = ['publisher', 'container-title', 'journal', 'date', 'location', 'note']
    candidate_text = ""

    # å¦‚æœæ¨™é¡Œæ˜¯ç©ºçš„ï¼Œæˆ–è€…æ¨™é¡Œçœ‹èµ·ä¾†åƒæ˜¯å¹´ä»½/ç·¨è™Ÿ (å¤ªçŸ­)
    if not title or len(title) < 5:
        for field in garbage_fields:
            val = item.get(field)
            if val and isinstance(val, str) and len(val) > 10:
                # ç‰¹å¾µï¼šåŒ…å«å¹´ä»½æ‹¬è™Ÿ "2004)" æˆ– "RFC"
                if re.search(r'\d{4}.*?[)\]]\.?\s', val) or "RFC" in val:
                    candidate_text = val
                    break
        
        if candidate_text:
            # ç­–ç•¥ A: é‡å° "æ—¥æœŸ). æ¨™é¡Œ" çš„æ ¼å¼ (æ”¾å¯¬ Regex: \s+ æ”¹ç‚º \s*)
            match_a = re.search(r'\d{4}.*?[)\]]\.?\s*(.*?)(?=\s*[\(\[]RFC|\s*[\(\[]Online|\s*Avail|\s*$)', candidate_text, re.IGNORECASE)
            
            if match_a:
                extracted_title = match_a.group(1).strip()
                if len(extracted_title) > 3: # ç¢ºä¿æŠ“åˆ°çš„ä¸æ˜¯ç©ºå­—ä¸²
                    item['title'] = extracted_title
            
            # ç­–ç•¥ B: é‡å° RFC ç›´æ¥åˆ‡å‰²
            elif "RFC" in candidate_text:
                parts = candidate_text.split("RFC")
                potential_title = parts[0]
                potential_title = re.sub(r'[\(\[]$', '', potential_title).strip()
                potential_title = re.sub(r'^.*?\d{4}.*?[)\]]\.?\s*', '', potential_title).strip()
                if len(potential_title) > 5:
                    item['title'] = potential_title

    # 4. ç‰ˆæ¬¡/å‡ºç‰ˆç¤¾åˆ†é›¢
    if item.get('edition') and not item.get('publisher'):
        ed_text = item['edition']
        match = re.search(r'^([(\[]?.*?(?:ed\.|edition|edn)[)\]]?)\s*[:.,]?\s*(.+)$', ed_text, re.IGNORECASE)
        if match:
            item['edition'] = match.group(1).strip()       
            item['publisher'] = match.group(2).strip(' .,') 
    
    # 5. æ ¼å¼åŒ–äººå
    if item.get('authors'): item['authors'] = format_name_field(item['authors'])
    if item.get('editor'): item['editor'] = format_name_field(item['editor'])
    
    return item

# ========== å´é‚Šæ¬„ ==========
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    st.subheader("ğŸ“‚ æœ¬åœ°è³‡æ–™åº«")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥: {len(local_df)} ç­†")
            default_idx = 0
            if "è«–æ–‡åç¨±" in local_df.columns: default_idx = list(local_df.columns).index("è«–æ–‡åç¨±")
            target_col = st.selectbox("æ¯”å°æ¬„ä½:", options=local_df.columns, index=default_idx, disabled=True)
    else:
        st.error(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {DEFAULT_CSV_PATH}")
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"Scopus API: {'âœ… å·²è¼‰å…¥' if scopus_key else 'âŒ æœªè¨­å®š'}")
    st.info(f"SerpAPI: {'âœ… å·²è¼‰å…¥' if serpapi_key else 'âŒ æœªè¨­å®š'}")
    
    check_crossref = True
    check_scopus = True
    check_openalex = True
    check_s2 = True
    check_scholar = True

# ========== ä¸»é‚è¼¯ ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥èˆ‡è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    raw_input = st.text_area("åœ¨æ­¤è²¼ä¸Šå…§å®¹...", height=300)
    
    if st.button("ğŸš€ ä½¿ç”¨ AnyStyle è§£æ", type="primary"):
        if not raw_input:
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            with st.spinner("æ­£åœ¨å‘¼å« Docker å®¹å™¨é€²è¡Œè§£æ..."):
                raw_list, struct_list = parse_references_with_anystyle(raw_input)
            if struct_list:
                st.session_state.structured_references = struct_list
                print(struct_list)
                st.success(f"âœ… è§£ææˆåŠŸï¼å…± {len(struct_list)} ç­†ã€‚")
            else:
                st.error("âŒ AnyStyle æœ¬æ©Ÿè§£æå¤±æ•—ï¼Œè«‹ç¢ºèª Ruby / anystyle-cli æ˜¯å¦æ­£ç¢ºå®‰è£ã€‚")

with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è¼¸å…¥ä¸¦è§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰æ–‡ç» (å¾ªåºæ¨¡å¼)", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_sequential(idx, raw_ref):
                # 1. å¼·åŠ›æ¸…æ´—èˆ‡æ¬„ä½ä¿®æ­£ (DOI æ¬å®¶ç™¼ç”Ÿåœ¨é€™è£¡)
                ref = refine_parsed_data(raw_ref)
                
                title = ref.get('title', '')
                text = ref.get('text', '')
                doi = ref.get('doi')     # å·²ç¶“å¾ URL æ•‘å›ä¾†äº†
                parsed_url = ref.get('url')
                
                # æå–ç¬¬ä¸€ä½œè€… (ç”¨æ–¼è¼”åŠ©æœå°‹)
                first_author = ""
                if ref.get('authors'):
                    auth_raw = ref['authors'].split(';')[0].split(',')[0]
                    first_author = auth_raw[:20].strip()

                res = {
                    "id": idx, "title": title, "text": text, "parsed": ref,
                    "sources": {}, "found_at_step": None
                }

                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', title)) if title else False

                # Step 0: Local DB
                if has_chinese and local_df is not None and target_col and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "æœ¬åœ°è³‡æ–™åº«åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref (DOI or Text)
                if check_crossref:
                    if doi:
                        _, url = search_crossref_by_doi(doi)
                        if url:
                            res["sources"]["Crossref"] = url
                            res["found_at_step"] = "1. Crossref (DOI)"
                            return res
                    # ç„¡ DOIï¼Œå˜—è©¦æ–‡å­—æœå°‹
                    elif title and len(title) > 5:
                        url = search_crossref_by_text(title, first_author)
                        if url:
                            res["sources"]["Crossref"] = url
                            res["found_at_step"] = "1. Crossref (Text)"
                            return res

                # Step 2: Scopus
                if check_scopus and scopus_key and title:
                    url = search_scopus_by_title(title, scopus_key)
                    if url:
                        res["sources"]["Scopus"] = url
                        res["found_at_step"] = "2. Scopus"
                        return res 

                # Step 3: OpenAlex (Smart Fallback)
                if check_openalex and title:
                    url = search_openalex_by_title(title, first_author)
                    if url:
                        res["sources"]["OpenAlex"] = url
                        res["found_at_step"] = "3. OpenAlex"
                        return res 

                # Step 4: Semantic Scholar (Smart Fallback)
                if check_s2 and title:
                    url = search_s2_by_title(title, first_author)
                    if url:
                        res["sources"]["Semantic Scholar"] = url
                        res["found_at_step"] = "4. Semantic Scholar"
                        return res 

                # Step 5: Google Scholar
                if check_scholar and serpapi_key:
                    if title:
                        url, status = search_scholar_by_title(title, serpapi_key)
                        if status in ["match", "similar"]:
                            res["sources"]["Google Scholar"] = url
                            res["found_at_step"] = "5. Scholar (Title)"
                            return res 
                    
                    url_r, status_r = search_scholar_by_ref_text(text, serpapi_key)
                    if status_r != "no_result":
                        res["sources"]["Google Scholar (è£œæ•‘)"] = url_r
                        res["found_at_step"] = "5. Scholar (Text)"
                        return res 

                # Step 6: Website Check
                # [ä¿®æ­£] åš´æ ¼ç¶²ç«™æª¢æŸ¥ï¼š
                # 1. å¿…é ˆæ˜¯ http é–‹é ­
                # 2. ä¸èƒ½åŒ…å« 'doi.org' (å› ç‚ºé‚£æ˜¯è«–æ–‡é€£çµ)
                # 3. ä¸èƒ½åŒ…å« '10.xxxx/' (é¿å…æ¼ç¶²çš„ DOI)
                if parsed_url and parsed_url.startswith('http'):
                    is_doi_link = 'doi.org' in parsed_url or re.search(r'10\.\d{4}/', parsed_url)
                    
                    if not is_doi_link:
                        is_valid = check_url_availability(parsed_url)
                        if is_valid:
                            res["sources"]["Direct Link"] = parsed_url
                            res["found_at_step"] = "6. Website / Direct URL"
                            return res
                        else:
                            res["sources"]["Direct Link (Dead)"] = parsed_url
                            res["found_at_step"] = "6. Website (Link Failed)" 
                            return res

                return res

            max_workers = min(5, total)
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(check_single_sequential, i+1, r): i for i, r in enumerate(refs)}
                for i, future in enumerate(as_completed(futures)):
                    try:
                        data = future.result()
                        results_buffer.append(data)
                        progress.progress((i + 1) / total)
                        status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")
                    except Exception as e:
                        st.error(f"Error on item {i}: {e}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            status_text.success("âœ… é©—è­‰å®Œæˆï¼")
            time.sleep(1)
            st.rerun()

        if st.session_state.results:
            st.divider()
            col1, col2 = st.columns([1, 3])
            with col1:
                filter_option = st.selectbox(
                    "ğŸ“‚ ç¯©é¸é¡¯ç¤ºçµæœ",
                    ["å…¨éƒ¨é¡¯ç¤º", "âœ… è³‡æ–™åº«é©—è­‰", "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº", "âš ï¸ ç¶²ç«™ (é€£ç·šå¤±æ•—)", "âŒ æœªæ‰¾åˆ°çµæœ"],
                    index=0
                )
            
            verified_db_count = sum(1 for r in st.session_state.results if r.get('found_at_step') and "Website" not in r.get('found_at_step'))
            valid_web_count = sum(1 for r in st.session_state.results if r.get('found_at_step') == "6. Website / Direct URL")
            failed_web_count = sum(1 for r in st.session_state.results if r.get('found_at_step') == "6. Website (Link Failed)")
            unverified_count = len(st.session_state.results) - (verified_db_count + valid_web_count + failed_web_count)
            
            with col2:
                st.markdown(f"""
                <div style="padding-top: 10px;">
                    <span class="status-badge" style="background:#D1FAE5; color:#065F46;">ğŸ“š è³‡æ–™åº«: {verified_db_count}</span>
                    <span class="status-badge" style="background:#DBEAFE; color:#1E40AF;">ğŸŒ æœ‰æ•ˆç¶²ç«™: {valid_web_count}</span>
                    <span class="status-badge" style="background:#FEF3C7; color:#92400E;">âš ï¸ ç¶²ç«™(Fail): {failed_web_count}</span>
                    <span class="status-badge" style="background:#FEE2E2; color:#991B1B;">âŒ æœªæ‰¾åˆ°: {unverified_count}</span>
                </div>
                """, unsafe_allow_html=True)

            st.divider()

            for res in st.session_state.results:
                found_step = res.get('found_at_step')
                
                is_db_verified = found_step and "Website" not in found_step
                is_web_valid = found_step == "6. Website / Direct URL"
                is_web_failed = found_step == "6. Website (Link Failed)"
                
                if filter_option == "âœ… è³‡æ–™åº«é©—è­‰" and not is_db_verified: continue
                if filter_option == "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº" and not is_web_valid: continue
                if filter_option == "âš ï¸ ç¶²ç«™ (é€£ç·šå¤±æ•—)" and not is_web_failed: continue
                if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and (is_db_verified or is_web_valid or is_web_failed): continue

                bg_color = "#FEE2E2"
                if is_db_verified: bg_color = "#D1FAE5"
                elif is_web_valid: bg_color = "#DBEAFE"
                elif is_web_failed: bg_color = "#FEF3C7"
                
                status_label = f"âœ… {found_step}" if is_db_verified else (f"ğŸŒ {found_step}" if is_web_valid else (f"âš ï¸ {found_step}" if is_web_failed else "âŒ æœªæ‰¾åˆ°"))
                
                p = res.get('parsed', {})
                with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                    st.markdown(f"""<div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-bottom: 15px;"><b>ç‹€æ…‹:</b> {status_label}</div>""", unsafe_allow_html=True)
                    
                    display_author = p.get('authors') or (f"{p['editor']} (Ed.)" if p.get('editor') else "N/A")
                    display_title = p.get('title', 'N/A') + (f" {p['edition']}" if p.get('edition') else "")
                    source_parts = [x for x in [p.get('container-title'), p.get('journal'), f"{p.get('location')}: {p.get('publisher')}" if p.get('publisher') else p.get('publisher')] if x]
                    display_source = ", ".join(source_parts) if source_parts else "N/A"
                    
                    st.markdown(f"""
                    | | |
                    | :--- | :--- |
                    | **ğŸ‘¥ ä½œè€…/ç·¨è€…** | `{display_author}` |
                    | **ğŸ“… ç™¼è¡¨å¹´ä»½** | `{p.get('date', 'N/A')}` |
                    | **ğŸ“° æ–‡ç»æ¨™é¡Œ** | `{display_title}` |
                    | **ğŸ¢ å‡ºè™•/ç™¼è¡Œ** | `{display_source}` |
                    | **ğŸ”¢ DOI/URL** | `{p.get('doi', p.get('url', 'N/A'))}` |
                    """)
                    st.divider()
                    st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                    st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                    
                    if res['sources']:
                        st.write("**ğŸ”— é©—è­‰ä¾†æºé€£çµï¼š**")
                        for src, link in res['sources'].items():
                            if src == "Direct Link": st.markdown(f"- ğŸŒ **åŸå§‹ç¶²ç«™ (å·²æ¸¬è©¦å¯é€£ç·š)**: [é»æ“Šå‰å¾€]({link})")
                            elif src == "Direct Link (Dead)": st.markdown(f"- âš ï¸ **åŸå§‹ç¶²ç«™ (é€£ç·šé€¾æ™‚/å¤±æ•—ï¼Œè«‹æ‰‹å‹•ç¢ºèª)**: [é»æ“Šå‰å¾€]({link})")
                            elif link.startswith("http"): st.markdown(f"- **{src}**: [é»æ“Šé–‹å•Ÿ]({link})")
                            else: st.markdown(f"- **{src}**: {link}")
                    else:
                        st.warning("åœ¨æ‰€æœ‰å•Ÿç”¨çš„è³‡æ–™åº«ä¸­çš†æœªæ‰¾åˆ°åŒ¹é…é …ã€‚")

with tab3:
    if st.session_state.results:
        df = pd.DataFrame(st.session_state.results)
        df['Source'] = df['found_at_step'].fillna('Not Found')
        total = len(df)
        verified_count = len(df[df['Source'] != 'Not Found'])
        col1, col2 = st.columns(2)
        col1.metric("ç¸½æ–‡ç»æ•¸", total)
        col2.metric("å·²è­˜åˆ¥ä¾†æºæ•¸ (å«ç¶²ç«™)", verified_count, f"{verified_count/total*100:.1f}%")
        st.subheader("é©—è­‰ä¾†æºåˆ†ä½ˆ")
        st.bar_chart(df['Source'].value_counts())
        
        st.subheader("è©³ç´°è³‡æ–™è¡¨")
        export_data = []
        for r in st.session_state.results:
            row = r['parsed'].copy()
            row['id'] = r['id']
            row['verified_source'] = r.get('found_at_step', 'Not Found')
            row['verified_url'] = list(r['sources'].values())[0] if r['sources'] else ''
            export_data.append(row)
        st.dataframe(pd.DataFrame(export_data), use_container_width=True)
        csv = pd.DataFrame(export_data).to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š CSV", csv, "report.csv", "text/csv")
    else:
        st.info("å°šç„¡æ•¸æ“š")