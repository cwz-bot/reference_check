# app.py

import streamlit as st
import pandas as pd
import time
import os
import re  # <--- [æ–°å¢] ç”¨æ–¼æ­£è¦è¡¨é”å¼åˆ¤æ–·ä¸­æ–‡
from concurrent.futures import ThreadPoolExecutor, as_completed

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from modules.parsers import parse_references_with_anystyle
# å°å…¥æœ¬åœ°è³‡æ–™åº«æ¨¡çµ„
from modules.local_db import load_csv_data, search_local_database
# å°å…¥ API å®¢æˆ¶ç«¯
from modules.api_clients import (
    get_scopus_key,
    get_serpapi_key,
    search_crossref_by_doi,
    search_scopus_by_title,
    search_scholar_by_title,
    search_scholar_by_ref_text,
    search_s2_by_title,
    search_openalex_by_title
)

# ========== é é¢è¨­å®š ==========
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (Local DB + Docker)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
    .ref-box { background-color: #f8f9fa; padding: 10px; border-radius: 5px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    
    /* è¡¨æ ¼æ¨£å¼å„ªåŒ–ï¼šéš±è—è¡¨é ­ */
    div[data-testid="stMarkdownContainer"] table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 10px;
    }
    div[data-testid="stMarkdownContainer"] td {
        padding: 8px 5px;
        border-bottom: 1px solid #f0f0f0;
        font-size: 0.95em;
    }
    div[data-testid="stMarkdownContainer"] th {
        display: none; /* éš±è—è¡¨é ­ */
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (æ··åˆé›²åœ°ç‰ˆ)</div>', unsafe_allow_html=True)

# ========== Session State ==========
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ========== å´é‚Šæ¬„ ==========
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # --- 1. æœ¬åœ°è³‡æ–™åº«è¨­å®š (è‡ªå‹•è¼‰å…¥é è¨­æª”ï¼Œéš±è—ä¸Šå‚³å€) ---
    st.subheader("ğŸ“‚ æœ¬åœ°è³‡æ–™åº« (å„ªå…ˆæª¢æŸ¥)")
    
    DEFAULT_CSV_PATH = "112ndltd.csv" # é–å®šé è¨­æª”æ¡ˆ
    local_df = None
    target_col = None
    
    # ç›´æ¥æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸¦è¼‰å…¥
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file):
            return load_csv_data(file)

        local_df = read_data_cached(DEFAULT_CSV_PATH)
        
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥å…§å»ºè³‡æ–™åº«: {len(local_df)} ç­†è³‡æ–™")
            
            # è‡ªå‹•åµæ¸¬æ¨™é¡Œæ¬„ä½
            default_idx = 0
            if "è«–æ–‡åç¨±" in local_df.columns:
                default_idx = list(local_df.columns).index("è«–æ–‡åç¨±")
            
            target_col = st.selectbox(
                "æ¯”å°æ¬„ä½:", # ç°¡åŒ–æ¨™ç±¤
                options=local_df.columns,
                index=default_idx,
                disabled=True # é¸é …ï¼šæ‚¨å¯ä»¥é–å®šé€™å€‹é¸å–®ä¸è®“äººæ”¹ï¼Œæˆ–è€…ä¿ç•™è®“ä½¿ç”¨è€…çœ‹
            )
            st.info("ğŸ’¡ ç³»çµ±å„ªå…ˆæœå°‹æœ¬åœ°åº« (é™ä¸­æ–‡æ–‡ç»)ï¼Œæ‰¾ä¸åˆ°æ‰è¯ç¶²ã€‚")
    else:
        st.error(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é è¨­æª”æ¡ˆ {DEFAULT_CSV_PATH}")
        st.warning("è«‹ç¢ºèªæª”æ¡ˆå·²æ”¾å…¥å°ˆæ¡ˆè³‡æ–™å¤¾ä¸­ã€‚")
    
    st.divider()

    # --- 2. API ç‹€æ…‹ ---
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    
    st.info(f"Scopus API: {'âœ… å·²è¼‰å…¥' if scopus_key else 'âŒ æœªè¨­å®š'}")
    st.info(f"SerpAPI: {'âœ… å·²è¼‰å…¥' if serpapi_key else 'âŒ æœªè¨­å®š'}")
    
    st.divider()
    
    # --- 3. æª¢æŸ¥é †åºèªªæ˜ ---
    st.subheader("ğŸ” æª¢æŸ¥é †åº")
    st.markdown("""
    ç³»çµ±å°‡ä¾åºæª¢æŸ¥ç›´åˆ°æ‰¾åˆ°çµæœï¼š
    1. **æœ¬åœ° CSV è³‡æ–™åº«** (åƒ…é™ä¸­æ–‡)
    2. **Crossref** (DOI)
    3. **Scopus**
    4. **OpenAlex**
    5. **Semantic Scholar**
    6. **Google Scholar**
    """)
    
    # å¼·åˆ¶é–‹å•Ÿæ‰€æœ‰ç·šä¸Šæª¢æŸ¥
    check_crossref = True
    check_scopus = True
    check_openalex = True
    check_s2 = True
    check_scholar = True

# ========== ä¸»é‚è¼¯ ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥èˆ‡è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

# --- TAB 1: è¼¸å…¥ ---
with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    st.info("ğŸ’¡ è«‹ç›´æ¥è²¼ä¸Šæ•´æ®µæ–‡ç»ï¼ŒDocker å®¹å™¨å…§çš„ AnyStyle æœƒè‡ªå‹•è­˜åˆ¥ä¸¦æ‹†åˆ†ã€‚")
    raw_input = st.text_area("åœ¨æ­¤è²¼ä¸Šå…§å®¹...", height=300)
    
    if st.button("ğŸš€ ä½¿ç”¨ AnyStyle è§£æ", type="primary"):
        if not raw_input:
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            
            with st.spinner("æ­£åœ¨å‘¼å« Docker å®¹å™¨é€²è¡Œè§£æ..."):
                raw_list, struct_list = parse_references_with_anystyle(raw_input)
            
            if struct_list:
                st.session_state.structured_references = struct_list
                st.success(f"âœ… è§£ææˆåŠŸï¼å…±è­˜åˆ¥å‡º {len(struct_list)} ç­†æ–‡ç»ã€‚è«‹åˆ‡æ›è‡³ã€Œé©—è­‰çµæœã€é é¢ã€‚")
                with st.expander("é è¦½è§£æç´°ç¯€ (JSON)"):
                    st.json(struct_list[:3])
            else:
                st.error("è§£æå¤±æ•—ï¼Œè«‹ç¢ºèª Docker æ˜¯å¦æ­£åœ¨åŸ·è¡Œã€‚")

# --- TAB 2: æª¢æŸ¥ ---
with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è¼¸å…¥ä¸¦è§£ææ–‡ç»ã€‚")
    else:
        # é–‹å§‹æª¢æŸ¥æŒ‰éˆ•
        if st.button("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰æ–‡ç» (å¾ªåºæ¨¡å¼)", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            # å®šç¾©å–®ç­†æª¢æŸ¥å‡½å¼
            def check_single_sequential(idx, ref):
                title = ref.get('title', '')
                text = ref.get('text', '')
                doi = ref.get('doi')
                
                res = {
                    "id": idx,
                    "title": title,
                    "text": text,
                    "parsed": ref, # ä¿å­˜è§£æè³‡æ–™
                    "sources": {},
                    "found_at_step": None
                }
                
                # --- [æ–°å¢] èªè¨€åˆ¤æ–·é‚è¼¯ ---
                # åˆ¤æ–·æ¨™é¡Œæ˜¯å¦åŒ…å«ä¸­æ–‡å­—å…ƒ (Unicode ç¯„åœ 4E00-9FFF)
                # å¦‚æœæ²’æœ‰æ¨™é¡Œï¼Œå‰‡é è¨­ä¸å«ä¸­æ–‡ (False)
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', title)) if title else False

                # ğŸ›‘ Step 0: æœ¬åœ° CSV è³‡æ–™åº« (æœ€å„ªå…ˆ)
                # åªæœ‰ç•¶æ¨™é¡ŒåŒ…å«ä¸­æ–‡æ™‚ï¼Œæ‰æœå°‹æœ¬åœ°è³‡æ–™åº«
                if has_chinese and local_df is not None and target_col and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "æœ¬åœ°è³‡æ–™åº«åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref
                if check_crossref and doi:
                    _, url = search_crossref_by_doi(doi)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref"
                        return res 

                # Step 2: Scopus
                if check_scopus and scopus_key and title:
                    url = search_scopus_by_title(title, scopus_key)
                    if url:
                        res["sources"]["Scopus"] = url
                        res["found_at_step"] = "2. Scopus"
                        return res 

                # Step 3: OpenAlex
                if check_openalex and title:
                    url = search_openalex_by_title(title)
                    if url:
                        res["sources"]["OpenAlex"] = url
                        res["found_at_step"] = "3. OpenAlex"
                        return res 

                # Step 4: Semantic Scholar
                if check_s2 and title:
                    url = search_s2_by_title(title)
                    if url:
                        res["sources"]["Semantic Scholar"] = url
                        res["found_at_step"] = "4. Semantic Scholar"
                        return res 

                # Step 5: Google Scholar
                if check_scholar and serpapi_key:
                    if title:
                        url, status = search_scholar_by_title(title, serpapi_key)
                        if status in ["match", "similar"]:
                            res["sources"]["Google Scholar"] = url
                            res["found_at_step"] = "5. Scholar (Title)"
                            return res 
                    
                    url_r, status_r = search_scholar_by_ref_text(text, serpapi_key)
                    if status_r != "no_result":
                        res["sources"]["Google Scholar (è£œæ•‘)"] = url_r
                        res["found_at_step"] = "5. Scholar (Text)"
                        return res 

                return res

            # å¤šåŸ·è¡Œç·’åŸ·è¡Œ
            max_workers = min(5, total)
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(check_single_sequential, i+1, r): i for i, r in enumerate(refs)}
                
                for i, future in enumerate(as_completed(futures)):
                    try:
                        data = future.result()
                        results_buffer.append(data)
                        progress.progress((i + 1) / total)
                        status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")
                    except Exception as e:
                        st.error(f"Error on item {i}: {e}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            status_text.success("âœ… é©—è­‰å®Œæˆï¼")
            time.sleep(1)
            st.rerun()

        # ======================================================
        # é¡¯ç¤ºçµæœ (å«ç¯©é¸åŠŸèƒ½)
        # ======================================================
        if st.session_state.results:
            st.divider()
            
            # ç¯©é¸é¸å–®
            col1, col2 = st.columns([1, 3])
            with col1:
                filter_option = st.selectbox(
                    "ğŸ“‚ ç¯©é¸é¡¯ç¤ºçµæœ",
                    ["å…¨éƒ¨é¡¯ç¤º", "âœ… å·²é©—è­‰æˆåŠŸ", "âŒ æœªæ‰¾åˆ°çµæœ"],
                    index=0
                )
            
            # çµ±è¨ˆæ•¸æ“š
            verified_count = sum(1 for r in st.session_state.results if r.get('found_at_step'))
            unverified_count = len(st.session_state.results) - verified_count
            with col2:
                st.caption(f"ç¸½è¨ˆ: {len(st.session_state.results)} | âœ… å·²é©—è­‰: {verified_count} | âŒ æœªæ‰¾åˆ°: {unverified_count}")

            st.divider()

            # çµæœè¿´åœˆ
            for res in st.session_state.results:
                found_step = res.get('found_at_step')
                is_verified = found_step is not None
                
                # --- ç¯©é¸é‚è¼¯ ---
                if filter_option == "âœ… å·²é©—è­‰æˆåŠŸ" and not is_verified: continue
                if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and is_verified: continue
                # ----------------

                status_label = f"âœ… {found_step}" if found_step else "âŒ æœªæ‰¾åˆ°"
                bg_color = "#D1FAE5" if found_step else "#FEE2E2"
                
                p = res.get('parsed', {})

                with st.expander(f"{res['id']}. {res['title'][:80]}..."):
                    # 1. ç‹€æ…‹åˆ—
                    st.markdown(f"""
                    <div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-bottom: 15px;">
                        <b>ç‹€æ…‹:</b> {status_label}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # 2. è©³ç´°æ¬„ä½è³‡æ–™ (è¡¨æ ¼)
                    st.markdown(f"""
                    | | |
                    | :--- | :--- |
                    | **ğŸ‘¥ ä½œè€…** | `{p.get('authors', 'N/A')}` |
                    | **ğŸ“… å¹´ä»½** | `{p.get('date', 'N/A')}` |
                    | **ğŸ“° æ¨™é¡Œ** | `{p.get('title', 'N/A')}` |
                    | **ğŸ“– æœŸåˆŠ** | `{p.get('container-title', p.get('journal', 'N/A'))}` |
                    | **ğŸ”¢ DOI** | `{p.get('doi', 'N/A')}` |
                    """)
                    
                    st.divider()

                    # 3. åŸå§‹æ–‡ç»èˆ‡é€£çµ
                    st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                    st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                    
                    if res['sources']:
                        st.write("**ğŸ”— é©—è­‰ä¾†æºé€£çµï¼š**")
                        for src, link in res['sources'].items():
                            if link.startswith("http"):
                                st.markdown(f"- **{src}**: [é»æ“Šé–‹å•Ÿ]({link})")
                            else:
                                st.markdown(f"- **{src}**: {link}")
                    else:
                        st.warning("åœ¨æ‰€æœ‰å•Ÿç”¨çš„è³‡æ–™åº«ä¸­çš†æœªæ‰¾åˆ°åŒ¹é…é …ã€‚")

# --- TAB 3: çµ±è¨ˆ ---
with tab3:
    if st.session_state.results:
        df = pd.DataFrame(st.session_state.results)
        df['Source'] = df['found_at_step'].fillna('Not Found')
        
        total = len(df)
        verified_count = len(df[df['Source'] != 'Not Found'])
        
        col1, col2 = st.columns(2)
        col1.metric("ç¸½æ–‡ç»æ•¸", total)
        col2.metric("æˆåŠŸé©—è­‰æ•¸", verified_count, f"{verified_count/total*100:.1f}%")
        
        st.subheader("é©—è­‰ä¾†æºåˆ†ä½ˆ")
        st.bar_chart(df['Source'].value_counts())
        
        st.subheader("è©³ç´°è³‡æ–™è¡¨")
        export_data = []
        for r in st.session_state.results:
            row = r['parsed'].copy()
            row['id'] = r['id']
            row['verified_source'] = r.get('found_at_step', 'Not Found')
            row['verified_url'] = list(r['sources'].values())[0] if r['sources'] else ''
            export_data.append(row)
            
        st.dataframe(pd.DataFrame(export_data), use_container_width=True)
        
        csv = pd.DataFrame(export_data).to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š CSV", csv, "report.csv", "text/csv")
    else:
        st.info("å°šç„¡æ•¸æ“š")