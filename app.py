# app.py å®Œæ•´èåˆç‰ˆ

import streamlit as st
import pandas as pd
import time
import os
import re
import ast 
from concurrent.futures import ThreadPoolExecutor, as_completed

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from modules.parsers import parse_references_with_anystyle
# å°å…¥æœ¬åœ°è³‡æ–™åº«æ¨¡çµ„
from modules.local_db import load_csv_data, search_local_database
# å°å…¥ API å®¢æˆ¶ç«¯
from modules.api_clients import (
    get_scopus_key,
    get_serpapi_key,
    search_crossref_by_doi,
    search_crossref_by_text,
    search_scopus_by_title,
    search_scholar_by_title,
    search_scholar_by_ref_text, # ç¬¬äºŒæ®µæ–°å¢
    search_s2_by_title,
    search_openalex_by_title,
    check_url_availability
)

# ========== é é¢è¨­å®š ==========
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (ç³»çµ±å¢å¼·ç‰ˆ)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; margin-right: 5px; }
    .ref-box { background-color: #f8f9fa; padding: 10px; border-radius: 5px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    
    div[data-testid="stMarkdownContainer"] table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 10px;
    }
    div[data-testid="stMarkdownContainer"] td {
        padding: 8px 5px;
        border-bottom: 1px solid #f0f0f0;
        font-size: 0.95em;
    }
    div[data-testid="stMarkdownContainer"] th {
        display: none; 
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (è§£æè£œæ•‘èˆ‡å»ºè­°å¢å¼·ç‰ˆ)</div>', unsafe_allow_html=True)

# ========== Session State ==========
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ========== [è¼”åŠ©] 1. äººåæ ¼å¼åŒ– ==========
def format_name_field(data):
    if not data: return None
    if isinstance(data, str) and not (data.startswith('[') or data.startswith('{')): return data
    try:
        if isinstance(data, str):
            try: data = ast.literal_eval(data)
            except: return data
        names_list = []
        if isinstance(data, dict): data = [data]
        elif not isinstance(data, list): return str(data)
        for item in data:
            if isinstance(item, dict):
                parts = []
                if item.get('family'): parts.append(item['family'])
                if item.get('given'): parts.append(item['given'])
                if parts: names_list.append(", ".join(parts))
            else:
                names_list.append(str(item))
        return "; ".join(names_list)
    except:
        return str(data)

# ========== [æ ¸å¿ƒè£œæ•‘] 2. è³‡æ–™æ¸…æ´—èˆ‡æ¨™é¡Œæå–ä¿®æ­£ ==========
def refine_parsed_data(parsed_item):
    item = parsed_item.copy()
    raw_text = item.get('text', '').strip()

    # åŸºç¤ç¬¦è™Ÿæ¸…æ´—
    for key in ['doi', 'url', 'title', 'date']:
        if item.get(key) and isinstance(item[key], str):
            item[key] = item[key].strip(' ,.;)]}>')

    # --- [æ ¸å¿ƒè£œå¯«ï¼šè™•ç† StyleTTS 2 / AIOS ç­‰æ ¼å¼] ---
    title = item.get('title', '')
    
    if not title or len(title) < 10:
        # æ¨¡å¼ A: é‡å° "ç¸®å¯«: å®Œæ•´æ¨™é¡Œ"
        abbr_match = re.search(r'^([A-Z0-9\-\.\s]{2,12}:\s*.+?)(?=\s*[,\[]|\s*Available|\s*\(|\bhttps?://|\.|$)', raw_text)
        if abbr_match:
            item['title'] = abbr_match.group(1).strip()
        else:
            # æ¨¡å¼ B: AnyStyle æŠŠæ¨™é¡Œèª¤åˆ¤ç‚ºå‡ºç‰ˆå•†æˆ–æœŸåˆŠ
            for backup_key in ['publisher', 'container-title', 'journal']:
                val = item.get(backup_key)
                if val and len(str(val)) > 15:
                    item['title'] = str(val).strip()
                    break

    # DOI æå–
    url_val = item.get('url', '')
    if url_val:
        doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', url_val)
        if doi_match:
            item['doi'] = doi_match.group(1).strip('.')

    if item.get('authors'): item['authors'] = format_name_field(item['authors'])
    if item.get('editor'): item['editor'] = format_name_field(item['editor'])
    
    return item

# ========== å´é‚Šæ¬„ ==========
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥æœ¬åœ°åº«: {len(local_df)} ç­†")
            target_col = "è«–æ–‡åç¨±" if "è«–æ–‡åç¨±" in local_df.columns else local_df.columns[0]
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"Scopus API: {'âœ…' if scopus_key else 'âŒ'}")
    st.info(f"SerpAPI: {'âœ…' if serpapi_key else 'âŒ'}")

# ========== ä¸»é é¢ ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    raw_input = st.text_area("åœ¨æ­¤è¼¸å…¥æ–‡ç»å…§å®¹...", height=300, placeholder="ä¾‹å¦‚: StyleTTS 2: Towards Human-Level Text-to-Speech...")
    
    if st.button("ğŸš€ é–‹å§‹è§£æ", type="primary"):
        if not raw_input:
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            with st.spinner("AnyStyle è§£æä¸­..."):
                _, struct_list = parse_references_with_anystyle(raw_input)
            if struct_list:
                st.session_state.structured_references = struct_list
                st.success(f"âœ… è§£ææˆåŠŸï¼å…± {len(struct_list)} ç­†ã€‚")
            else:
                st.error("âŒ AnyStyle è§£æç•°å¸¸ã€‚")

with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹å…¨è‡ªå‹•é©—è­‰ (ä½µç™¼æ¨¡å¼)", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_task(idx, raw_ref):
                ref = refine_parsed_data(raw_ref)
                title = ref.get('title', '')
                text = ref.get('text', '')
                search_query = title if (title and len(title) > 8) else text[:120]
                
                doi = ref.get('doi')
                parsed_url = ref.get('url')
                first_author = ref['authors'].split(';')[0].split(',')[0].strip() if ref.get('authors') else ""

                res = {
                    "id": idx, "title": title if title else "è§£æå¤±æ•— (ä½¿ç”¨ä¿åº•æœç´¢)", 
                    "text": text, "parsed": ref,
                    "sources": {}, "found_at_step": None, "debug_logs": {},
                    "suggestion": None # èåˆå»ºè­°é€£çµ
                }

                # Step 0: Local DB
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', search_query))
                if has_chinese and local_df is not None and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref (DOI & Text)
                if doi:
                    _, url, status = search_crossref_by_doi(doi, target_title=title if title else None)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref (DOI)"
                        return res
                
                url, status = search_crossref_by_text(search_query, first_author)
                if url:
                    res["sources"]["Crossref"] = url
                    res["found_at_step"] = "1. Crossref (Search)"
                    return res

                # Step 3: Scopus
                if scopus_key:
                    url, status = search_scopus_by_title(search_query, scopus_key)
                    if url:
                        res["sources"]["Scopus"] = url
                        res["found_at_step"] = "2. Scopus"
                        return res

                # Step 4: OpenAlex / S2 / Scholar (Title)
                for api_func, step_name in [
                    (lambda: search_openalex_by_title(search_query, first_author), "3. OpenAlex"),
                    (lambda: search_s2_by_title(search_query, first_author), "4. Semantic Scholar"),
                    (lambda: search_scholar_by_title(search_query, serpapi_key), "5. Google Scholar")
                ]:
                    try:
                        url, status = api_func()
                        if url:
                            res["sources"][step_name.split(". ")[1]] = url
                            res["found_at_step"] = step_name
                            return res
                        res["debug_logs"][step_name] = status
                    except: pass

                # è£œæ•‘æ©Ÿåˆ¶ï¼šScholar Ref Text (å­˜å…¥ Suggestion)
                if serpapi_key:
                    url_r, status_r = search_scholar_by_ref_text(text, serpapi_key, target_title=title)
                    if url_r:
                        res["suggestion"] = url_r
                        res["debug_logs"]["Scholar (Suggestion)"] = "æ‰¾åˆ°ç›¸ä¼¼çµæœä½†æœªé”é©—è­‰æ¨™æº–"
                    else:
                        res["debug_logs"]["Scholar (Text)"] = status_r

                # Step 5: Website URL Check
                if parsed_url and parsed_url.startswith('http'):
                    if check_url_availability(parsed_url):
                        res["sources"]["Direct Link"] = parsed_url
                        res["found_at_step"] = "6. Website / Direct URL"
                    else:
                        res["sources"]["Direct Link (Dead)"] = parsed_url
                        res["found_at_step"] = "6. Website (Link Failed)"
                
                return res

            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(check_single_task, i+1, r): i for i, r in enumerate(refs)}
                for i, future in enumerate(as_completed(futures)):
                    results_buffer.append(future.result())
                    progress.progress((i + 1) / total)
                    status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            st.rerun()

    # --- çµæœå±•ç¤ºå€ ---
    if st.session_state.results:
        st.divider()
        
        # èåˆç¬¬äºŒæ®µçš„é€²éšç¯©é¸èˆ‡çµ±è¨ˆ
        verified_db_count = sum(1 for r in st.session_state.results if r.get('found_at_step') and "Website" not in r.get('found_at_step'))
        valid_web_count = sum(1 for r in st.session_state.results if r.get('found_at_step') == "6. Website / Direct URL")
        failed_web_count = sum(1 for r in st.session_state.results if r.get('found_at_step') == "6. Website (Link Failed)")
        unverified_count = len(st.session_state.results) - (verified_db_count + valid_web_count + failed_web_count)

        col_f1, col_f2 = st.columns([1, 2])
        with col_f1:
            filter_option = st.selectbox(
                "ğŸ“‚ ç¯©é¸é¡¯ç¤ºçµæœ",
                ["å…¨éƒ¨é¡¯ç¤º", "âœ… è³‡æ–™åº«é©—è­‰", "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº", "âš ï¸ ç¶²ç«™ (é€£ç·šå¤±æ•—)", "âŒ æœªæ‰¾åˆ°çµæœ"],
                index=0
            )
        with col_f2:
            st.markdown(f"""
            <div style="padding-top: 10px;">
                <span class="status-badge" style="background:#D1FAE5; color:#065F46;">ğŸ“š è³‡æ–™åº«: {verified_db_count}</span>
                <span class="status-badge" style="background:#DBEAFE; color:#1E40AF;">ğŸŒ æœ‰æ•ˆç¶²ç«™: {valid_web_count}</span>
                <span class="status-badge" style="background:#FEF3C7; color:#92400E;">âš ï¸ ç¶²ç«™(Fail): {failed_web_count}</span>
                <span class="status-badge" style="background:#FEE2E2; color:#991B1B;">âŒ æœªæ‰¾åˆ°: {unverified_count}</span>
            </div>
            """, unsafe_allow_html=True)

        st.divider()

        for res in st.session_state.results:
            found_step = res.get('found_at_step')
            is_db_verified = found_step and "Website" not in found_step
            is_web_valid = found_step == "6. Website / Direct URL"
            is_web_failed = found_step == "6. Website (Link Failed)"
            
            # ç¯©é¸é‚è¼¯èåˆ
            if filter_option == "âœ… è³‡æ–™åº«é©—è­‰" and not is_db_verified: continue
            if filter_option == "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº" and not is_web_valid: continue
            if filter_option == "âš ï¸ ç¶²ç«™ (é€£ç·šå¤±æ•—)" and not is_web_failed: continue
            if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and (is_db_verified or is_web_valid or is_web_failed): continue

            bg_color = "#FEE2E2"
            if is_db_verified: bg_color = "#D1FAE5"
            elif is_web_valid: bg_color = "#DBEAFE"
            elif is_web_failed: bg_color = "#FEF3C7"
            
            status_label = f"âœ… {found_step}" if is_db_verified else (f"ğŸŒ {found_step}" if is_web_valid else (f"âš ï¸ {found_step}" if is_web_failed else "âŒ æœªæ‰¾åˆ°"))
            
            p = res.get('parsed', {})
            with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                st.markdown(f"""<div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-bottom: 15px;"><b>ç‹€æ…‹:</b> {status_label}</div>""", unsafe_allow_html=True)
                
                display_author = p.get('authors') or (f"{p['editor']} (Ed.)" if p.get('editor') else "N/A")
                display_title = p.get('title', 'N/A') + (f" {p['edition']}" if p.get('edition') else "")
                source_parts = [x for x in [p.get('container-title'), p.get('journal'), f"{p.get('location')}: {p.get('publisher')}" if p.get('publisher') else p.get('publisher')] if x]
                display_source = ", ".join(source_parts) if source_parts else "N/A"
                
                st.markdown(f"""
                | | |
                | :--- | :--- |
                | **ğŸ‘¥ ä½œè€…/ç·¨è€…** | `{display_author}` |
                | **ğŸ“… ç™¼è¡¨å¹´ä»½** | `{p.get('date', 'N/A')}` |
                | **ğŸ“° æ–‡ç»æ¨™é¡Œ** | `{display_title}` |
                | **ğŸ¢ å‡ºè™•/ç™¼è¡Œ** | `{display_source}` |
                | **ğŸ”¢ DOI/URL** | `{p.get('doi', p.get('url', 'N/A'))}` |
                """)
                
                st.divider()
                st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                
                # èåˆå»ºè­°é€£çµ (Suggestion)
                if res.get("suggestion"):
                    st.warning("ğŸ’¡ **è¼¸å…¥å¯èƒ½æœ‰èª¤ï¼Œç³»çµ±å»ºè­°ï¼š**")
                    st.markdown(f"ç³»çµ±åœ¨æ¨¡ç³Šæœå°‹ä¸­æ‰¾åˆ°äº†ç›¸ä¼¼æ–‡ç»ï¼Œè«‹ç¢ºèªï¼š\n\nğŸ‘‰ **[é»æ“ŠæŸ¥çœ‹ Google Scholar å»ºè­°çµæœ]({res['suggestion']})**")
                    st.caption("æ³¨æ„ï¼šæ­¤é …æœªè¢«æ­£å¼æ¨™è¨˜ç‚ºé©—è­‰æˆåŠŸã€‚")
                    st.divider()
                
                if res['sources']:
                    st.write("**ğŸ”— é©—è­‰ä¾†æºé€£çµï¼š**")
                    for src, link in res['sources'].items():
                        if src == "Direct Link": st.markdown(f"- ğŸŒ **åŸå§‹ç¶²ç«™ (å·²æ¸¬è©¦)**: [é»æ“Šå‰å¾€]({link})")
                        elif src == "Direct Link (Dead)": st.markdown(f"- âš ï¸ **åŸå§‹ç¶²ç«™ (é€£ç·šå¤±æ•—)**: [é»æ“Šå‰å¾€]({link})")
                        elif link.startswith("http"): st.markdown(f"- **{src}**: [é»æ“Šé–‹å•Ÿ]({link})")
                        else: st.markdown(f"- **{src}**: {link}")
                else:
                    st.error("âš ï¸ è³‡æ–™åº«çš†æœªæ‰¾åˆ°åŒ¹é…é …ã€‚")
                    with st.expander("ğŸ” æŸ¥çœ‹ Debug Logs"):
                        for api, msg in res.get("debug_logs", {}).items():
                            st.write(f"**{api}**: {msg}")

with tab3:
    if st.session_state.results:
        df_res = pd.DataFrame(st.session_state.results)
        st.metric("ç¸½æŸ¥æ ¸æ•¸", len(df_res))
        st.subheader("é©—è­‰ä¾†æºåˆ†ä½ˆ")
        st.bar_chart(df_res['found_at_step'].fillna('Not Found').value_counts())
        
        csv = df_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š CSV", csv, "report.csv", "text/csv")
    else:
        st.info("å°šç„¡çµ±è¨ˆæ•¸æ“š")