# app.py

import streamlit as st
import urllib.parse
import pandas as pd
from datetime import datetime
from io import StringIO
import re # re é‚„æ˜¯å¯èƒ½éœ€è¦ï¼Œä¿ç•™

# ========== å¾ modules å°å…¥æ‰€æœ‰åŠŸèƒ½ ==========
from modules.api_clients import (
    get_scopus_key, get_serpapi_key, search_crossref_by_doi,
    search_scopus_by_title, search_scholar_by_title,
    search_scholar_by_ref_text
)
from modules.file_processors import (
    extract_paragraphs_from_docx, extract_paragraphs_from_pdf,
    extract_reference_section_improved, extract_reference_section_from_bottom,
    detect_and_split_ieee, merge_references_by_heads
)
from modules.parsers import (
    detect_reference_style, find_apa_matches,
    find_apalike_matches, split_multiple_apa_in_paragraph,
    extract_doi,
    get_reference_keys, extract_in_text_citations # [!] å°å…¥ get_reference_keys (è¤‡æ•¸)
)
from modules.ui_components import analyze_single_reference
# ==========================================


# ========== è®€å– API Keys ==========
SCOPUS_API_KEY = get_scopus_key()
SERPAPI_KEY = get_serpapi_key()


# ========== Streamlit UI ==========
st.set_page_config(page_title="Reference Checker", layout="centered")
if "start_query" not in st.session_state:
    st.session_state.start_query = False
if "query_results" not in st.session_state:
    st.session_state.query_results = None
st.title("ğŸ“š Reference Checker")

st.markdown("""
<div style="background-color: #fff9db; padding: 15px; border-left: 6px solid #f1c40f; border-radius: 6px;">
    <span style="font-size: 16px; font-weight: bold;">æ³¨æ„äº‹é …</span><br>
    <span style="font-size: 15px; color: #444;">
    ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰ DOI ç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ï¼Œåƒ…é‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚æœ¬ç³»çµ±åƒ…æä¾›åˆæ­¥ç¯©é¸åƒè€ƒï¼Œæ¯”å°å¾Œæ‡‰é€²è¡Œäººå·¥æ ¸å°ï¼Œä¸å¾—ç›´æ¥ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºå­¸è¡“å€«ç†åˆ¤æ–·çš„ä¾æ“šã€‚
    </span>
</div>
""", unsafe_allow_html=True)
st.markdown(" ")

# [!] å¼•ç”¨å¯©æ ¸çš„å‹¾é¸æ¡†
st.session_state.check_citations = st.checkbox("ğŸ”¬ **(Beta) åŸ·è¡Œå…§æ–‡èˆ‡æ–‡æœ«å¼•ç”¨æ¯”å°**", value=False)
if st.session_state.check_citations:
    st.info("å•Ÿç”¨å¼•ç”¨æ¯”å°ï¼šå°‡æœƒå˜—è©¦è§£æå…§æ–‡å¼•ç”¨ (å¦‚ [1] æˆ– (Author, YYYY)) ä¸¦èˆ‡æ–‡æœ«åˆ—è¡¨æ¯”å°ã€‚æ­¤åŠŸèƒ½ç‚º Beta ç‰ˆï¼Œå¯èƒ½ç„¡æ³•å®Œç¾è§£ææ‰€æœ‰æ ¼å¼ã€‚")


uploaded_files = st.file_uploader("è«‹ä¸Šå‚³æœ€å¤š 10 å€‹ Word æˆ– PDF æª”æ¡ˆ", type=["docx", "pdf"], accept_multiple_files=True)
if uploaded_files and len(uploaded_files) > 10:
    st.error("âŒ ä¸Šå‚³æª”æ¡ˆè¶…é 10 å€‹ï¼Œè«‹åˆªé™¤éƒ¨åˆ†æª”æ¡ˆå¾Œå†è©¦ä¸€æ¬¡ã€‚")
    st.stop()

start_button = st.button("ğŸš€ é–‹å§‹æŸ¥è©¢")

if uploaded_files and start_button:
    st.subheader("ğŸ“Š æ­£åœ¨æŸ¥è©¢ä¸­ï¼Œè«‹ç¨å€™...")

    all_results = []

    for uploaded_file in uploaded_files:
        file_ext = uploaded_file.name.split(".")[-1].lower()
        st.markdown(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š {uploaded_file.name}")

        file_progress = st.progress(0.0)
        scholar_logs = []

        # æª”æ¡ˆè§£æ
        if file_ext == "docx":
            paragraphs = extract_paragraphs_from_docx(uploaded_file)
        elif file_ext == "pdf":
            paragraphs = extract_paragraphs_from_pdf(uploaded_file)
        else:
            st.warning(f"âš ï¸ æª”æ¡ˆ {uploaded_file.name} æ ¼å¼ä¸æ”¯æ´ï¼Œå°‡ç•¥éã€‚")
            continue

        # ========== 1. æ“·å–å…§æ–‡èˆ‡åƒè€ƒæ–‡ç»å€æ®µ ==========
        body_paragraphs, reference_paragraphs, matched_keyword, matched_method = extract_reference_section_improved(paragraphs)

        if not reference_paragraphs and not matched_keyword:
            # Fallback
            body_paragraphs, reference_paragraphs, matched_keyword = extract_reference_section_from_bottom(paragraphs)
            matched_method = "æ¨™æº–æ¨™é¡Œè­˜åˆ¥ï¼ˆåº•éƒ¨ï¼‰"

        if not reference_paragraphs and not matched_keyword:
            st.error(f"âŒ ç„¡æ³•è­˜åˆ¥æª”æ¡ˆ {uploaded_file.name} çš„åƒè€ƒæ–‡ç»å€æ®µï¼Œå°‡æ¨™è¨˜æ–¼å ±å‘Šä¸­ã€‚")
            file_results = {
                "filename": uploaded_file.name, "no_reference_section": True,
                "title_pairs": [], "crossref_doi_hits": {}, "scopus_hits": {},
                "scholar_hits": {}, "scholar_similar": {}, "scholar_remedial": {},
                "not_found": [], "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "listed_but_not_cited": [], "cited_but_not_listed": [] # [!]
            }
            all_results.append(file_results)
            continue

        # ========== 2. åˆä½µåƒè€ƒæ–‡ç»æ®µè½ ==========
        if file_ext == "pdf":
            ieee_refs = detect_and_split_ieee(reference_paragraphs)
            merged_references = ieee_refs if ieee_refs else merge_references_by_heads(reference_paragraphs)
        else:
            merged_references = merge_references_by_heads(reference_paragraphs)

        # è£œä¸
        if len(merged_references) >= 2:
            first_style = detect_reference_style(merged_references[0])
            if first_style == "Unknown":
                merged_references[0] = merged_references[0].strip() + " " + merged_references[1].strip()
                del merged_references[1]

        # ========== 3. (UI) é¡¯ç¤ºåˆä½µå¾Œçš„äººå·¥æª¢æŸ¥åˆ—è¡¨ ==========
        with st.expander("æ“·å–åˆ°çš„åƒè€ƒæ–‡ç»æ®µè½ï¼ˆä¾›äººå·¥æª¢æŸ¥ï¼‰"):
            st.markdown(f"åƒè€ƒæ–‡ç»æ®µè½åµæ¸¬æ–¹å¼ï¼š**{matched_method}**")
            st.markdown(f"èµ·å§‹é—œéµæ®µè½ï¼š**{matched_keyword}**")
            for i, para in enumerate(merged_references, 1):
                st.markdown(f"**{i}.** {para}")

        # ========== 4. (æ–°åŠŸèƒ½) å¼•ç”¨å¯©æ ¸ ==========
        cited_keys = set()
        # [!] MODIFIED: New aggregation logic
        all_listed_keys = set()
        ref_to_keys_map = {} # e.g., {"1. Gao...": ["num:1", "apa:gao:2023"]}
        
        if st.session_state.check_citations:
            # 4a. æ“·å–å…§æ–‡å¼•ç”¨
            try:
                cited_keys = extract_in_text_citations(body_paragraphs)
            except Exception as e:
                st.warning(f"âš ï¸ å…§æ–‡å¼•ç”¨è§£æå¤±æ•—ï¼š{e}")
                
            # 4b. æ“·å–æ–‡æœ«ç´¢å¼•éµ (åœ¨ä¸‹ä¸€å€‹è¿´åœˆä¸­)
            
        # ========== 5. (UI) é€ç­†è§£æ & æŸ¥è©¢ ==========
        title_pairs = []
        crossref_doi_hits = {}
        scopus_hits = {}
        scholar_hits = {}
        scholar_similar = {}
        scholar_remedial = {}
        not_found = []

        with st.expander("é€ç­†åƒè€ƒæ–‡ç»è§£æçµæœï¼ˆåˆä½µå¾Œæ®µè½ + æ¨™é¡Œ + DOI + æ ¼å¼ï¼‰"):
            ref_index = 1
            for para in merged_references:
                # [!] åŸ·è¡Œå¼•ç”¨å¯©æ ¸çš„ B éƒ¨åˆ†ï¼šå»ºç«‹æ–‡æœ«ç´¢å¼•
                if st.session_state.check_citations:
                    try:
                        # [!] MODIFIED: å‘¼å«è¤‡æ•¸å‡½å¼ get_reference_keys
                        ref_keys = get_reference_keys(para) 
                        if ref_keys:
                            # [!] MODIFIED: New aggregation logic
                            ref_to_keys_map[para] = ref_keys
                            all_listed_keys.update(ref_keys)
                    except Exception as e:
                        st.warning(f"âš ï¸ æ–‡æœ«ç´¢å¼•éµè§£æå¤±æ•—ï¼š{e}")

                # åŸ·è¡Œç¾æœ‰é‚è¼¯ï¼šè§£ææ¨™é¡Œ
                apa_matches = find_apa_matches(para)
                apalike_matches = find_apalike_matches(para)
                total_valid_years = len(apa_matches) + len(apalike_matches)

                if total_valid_years >= 2:
                    sub_refs = split_multiple_apa_in_paragraph(para)
                    st.markdown(f"ğŸ” å¼·åˆ¶åˆ‡åˆ†æ®µè½ï¼ˆåŸå§‹æ®µè½å« {total_valid_years} å€‹å¹´ä»½ï¼‰ï¼š")
                    for sub_ref in sub_refs:
                        result = analyze_single_reference(sub_ref, ref_index)
                        if result:
                            title_pairs.append(result)
                        ref_index += 1
                else:
                    result = analyze_single_reference(para, ref_index)
                    if result:
                        title_pairs.append(result)
                    ref_index += 1

        # ========== 6. åŸ·è¡Œ API æŸ¥è©¢ ==========
        total_queries = len(title_pairs)
        for i, (ref, title) in enumerate(title_pairs, 1):
            doi = extract_doi(ref)
            if doi:
                title_from_doi, url = search_crossref_by_doi(doi)
                if title_from_doi:
                    crossref_doi_hits[ref] = url
                    if total_queries > 0:
                        file_progress.progress(i / total_queries)
                    continue

            url = search_scopus_by_title(title, SCOPUS_API_KEY)
            if url:
                scopus_hits[ref] = url
            else:
                gs_url, gs_type = search_scholar_by_title(title, SERPAPI_KEY)
                scholar_logs.append(f"Google Scholar å›å‚³é¡å‹ï¼š{gs_type} / æ¨™é¡Œï¼š{title}")
                if gs_type == "match":
                    scholar_hits[ref] = gs_url
                elif gs_type == "similar":
                    scholar_similar[ref] = gs_url
                elif gs_type == "error":
                    not_found.append(ref)
                else:
                    remedial_url, remedial_type = search_scholar_by_ref_text(ref, SERPAPI_KEY)
                    scholar_logs.append(f"Google Scholar å›å‚³é¡å‹ï¼šremedial_{remedial_type} / æ¨™é¡Œï¼š{title}")
                    if remedial_type == "remedial":
                        scholar_remedial[ref] = remedial_url
                    else:
                        not_found.append(ref)

            if total_queries > 0:
                file_progress.progress(i / total_queries)

        if scholar_logs:
            with st.expander("Google Scholar æŸ¥è©¢éç¨‹ç´€éŒ„"):
                for line in scholar_logs:
                    st.text(line)

        # ========== 7. (æ–°åŠŸèƒ½) è™•ç†å¼•ç”¨å¯©æ ¸çµæœ ==========
        cited_but_not_listed = []
        listed_but_not_cited = []
        if st.session_state.check_citations:
            # [!] MODIFIED: New aggregation logic
            
            # 1. å…§æ–‡å¼•ç”¨ï¼Œä½†æ–‡æœ«æœªåˆ—å‡º (Missing)
            cited_but_not_listed = sorted(list(cited_keys - all_listed_keys))
            
            # 2. æ–‡æœ«åˆ—å‡ºï¼Œä½†å…§æ–‡æœªå¼•ç”¨ (Unused)
            listed_but_not_cited_raw = []
            for ref_text, keys_for_this_ref in ref_to_keys_map.items():
                is_cited = False
                if not keys_for_this_ref:
                    is_cited = False # å¦‚æœæ–‡ç»é€£ key éƒ½æ²’æœ‰ï¼Œè¦–ç‚ºæœªå¼•ç”¨
                else:
                    for key in keys_for_this_ref:
                        if key in cited_keys:
                            is_cited = True # åªè¦æœ‰ä¸€å€‹ key åŒ¹é…ä¸Šï¼Œå°±è¦–ç‚ºå·²å¼•ç”¨
                            break
                if not is_cited:
                    listed_but_not_cited_raw.append(ref_text)
            
            listed_but_not_cited = sorted(listed_but_not_cited_raw)


        # ========== 8. å„²å­˜æ‰€æœ‰çµæœ ==========
        file_results = {
            "filename": uploaded_file.name,
            "no_reference_section": False, # [!]
            "title_pairs": title_pairs,
            "crossref_doi_hits": crossref_doi_hits,
            "scopus_hits": scopus_hits,
            "scholar_hits": scholar_hits,
            "scholar_similar": scholar_similar,
            "scholar_remedial": scholar_remedial,
            "not_found": not_found,
            "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "listed_but_not_cited": listed_but_not_cited, # [!]
            "cited_but_not_listed": cited_but_not_listed  # [!]
        }

        all_results.append(file_results)

    # æª”æ¡ˆè™•ç†å®Œç•¢ï¼Œå„²å­˜è‡³ session
    st.session_state.query_results = all_results

# ... (if st.session_state.get("serpapi_error") ... ä¿æŒä¸è®Š) ...
if st.session_state.get("serpapi_error"):
    st.warning(f"âš ï¸ Google Scholar æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{st.session_state['serpapi_error']}")


# ========== [ä¿®æ”¹] çµæœé¡¯ç¤º UI ==========

if st.session_state.query_results:
        st.markdown("---")
        st.subheader("ğŸ“Š æŸ¥è©¢çµæœåˆ†é¡")
        
        report_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        for result in st.session_state.query_results:
            uploaded_filename = result.get("filename", "æœªçŸ¥æª”æ¡ˆ")
            report_time = result.get("report_time", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            st.markdown(f"ğŸ“„ æª”æ¡ˆåç¨±ï¼š {uploaded_filename}")

            if result.get("no_reference_section"):
                st.error("âŒ æœªæ‰¾åˆ°åƒè€ƒæ–‡ç»å€æ®µï¼Œç„¡æ³•é€²è¡Œåˆ†æã€‚")
                continue

            # [!] å–å¾—æ–°èˆŠæ‰€æœ‰çµæœ
            not_found = result.get("not_found", [])
            crossref_doi_hits = result.get("crossref_doi_hits", {})
            scholar_similar = result.get("scholar_similar", {})
            scholar_remedial = result.get("scholar_remedial", {})
            scopus_hits = result.get("scopus_hits", {})
            scholar_hits = result.get("scholar_hits", {})
            listed_but_not_cited = result.get("listed_but_not_cited", [])
            cited_but_not_listed = result.get("cited_but_not_listed", [])
            
            matched_count = len(crossref_doi_hits) + len(scopus_hits) + len(scholar_hits) + len(scholar_remedial)
            
            # [!] å»ºç«‹ TABS
            tab_list = [
                f"ğŸŸ¢ å‘½ä¸­çµæœï¼ˆ{matched_count}ï¼‰",
                f"ğŸŸ¡ Google Scholar é¡ä¼¼æ¨™é¡Œï¼ˆ{len(scholar_similar)}ï¼‰",
                f"ğŸ”´ å‡æŸ¥ç„¡çµæœï¼ˆ{len(not_found)}ï¼‰"
            ]
            
            # [!] å‹•æ…‹åŠ å…¥æ–°çš„ TAB
            if st.session_state.check_citations:
                inconsistency_count = len(listed_but_not_cited) + len(cited_but_not_listed)
                tab_list.append(f"âš ï¸ å¼•ç”¨ä¸ä¸€è‡´ï¼ˆ{inconsistency_count}ï¼‰")

            tabs = st.tabs(tab_list)
            
            # Tab 1: å‘½ä¸­çµæœ
            with tabs[0]:
                if crossref_doi_hits:
                    with st.expander(f"\U0001F7E2 Crossref DOI å‘½ä¸­ï¼ˆ{len(crossref_doi_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(crossref_doi_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [DOI é€£çµ]({url})", unsafe_allow_html=True)
                # ... (å…¶ä»–å‘½ä¸­çµæœ scopus, scholar, remedial ä¿æŒä¸è®Š) ...
                if scopus_hits:
                    with st.expander(f"\U0001F7E2 Scopus æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scopus_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scopus_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scopus é€£çµ]({url})", unsafe_allow_html=True)
                if scholar_hits:
                    with st.expander(f"\U0001F7E2 Google Scholar æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scholar_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scholar_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)
                if scholar_remedial:
                    with st.expander(f"\U0001F7E2 Google Scholar è£œæ•‘å‘½ä¸­ï¼ˆ{len(scholar_remedial)}ï¼‰"):
                        for i, (title, url) in enumerate(scholar_remedial.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)
                if not (crossref_doi_hits or scopus_hits or scholar_hits or scholar_remedial):
                    st.info("æ²’æœ‰å‘½ä¸­ä»»ä½•åƒè€ƒæ–‡ç»ã€‚")

            # Tab 2: é¡ä¼¼æ¨™é¡Œ
            with tabs[1]:
                if scholar_similar:
                    for i, (title, url) in enumerate(scholar_similar.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [Google Scholar çµæœé€£çµ]({url})", unsafe_allow_html=True)
                            st.warning("âš ï¸ æ­¤ç‚ºç›¸ä¼¼æ¨™é¡Œï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºæ–‡ç»ã€‚")
                else:
                    st.info("ç„¡æ¨™é¡Œç›¸ä¼¼ä½†ä¸ä¸€è‡´çš„çµæœã€‚")

            # Tab 3: å‡æŸ¥ç„¡çµæœ
            with tabs[2]:
                if not_found:
                    for i, title in enumerate(not_found, 1):
                        scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
                        st.markdown(f"{i}. {title}  \nğŸ”— [Google Scholar æœå°‹]({scholar_url})", unsafe_allow_html=True)
                    st.markdown("ğŸ‘‰ è«‹è€ƒæ…®æ‰‹å‹•æœå°‹ Google Scholarã€‚")
                else:
                    st.success("æ‰€æœ‰æ¨™é¡Œçš†æˆåŠŸæŸ¥è©¢ï¼")

            # [!] Tab 4: å¼•ç”¨ä¸ä¸€è‡´ (æ–°åŠŸèƒ½)
            if st.session_state.check_citations:
                with tabs[3]:
                    st.info("æ­¤åŠŸèƒ½ç‚º Beta ç‰ˆã€‚å®ƒé€éè§£æä½œè€…ã€å¹´ä»½æˆ–æ•¸å­—ç·¨è™Ÿä¾†æ¯”å°ï¼Œå¯èƒ½ç„¡æ³•æŠ“åˆ°æ‰€æœ‰æ ¼å¼ã€‚")
                    
                    with st.expander(f"ğŸ”´ æ–‡æœ«åˆ—å‡ºï¼Œä½†å…§æ–‡æœªå¼•ç”¨ (Unused) ({len(listed_but_not_cited)})"):
                        if listed_but_not_cited:
                            for i, ref in enumerate(listed_but_not_cited, 1):
                                st.markdown(f"{i}. {ref}")
                        else:
                            st.success("æ‰€æœ‰æ–‡æœ«åƒè€ƒæ–‡ç»å‡åœ¨å…§æ–‡ä¸­è¢«å¼•ç”¨ã€‚")

                    with st.expander(f"ğŸŸ  å…§æ–‡å¼•ç”¨ï¼Œä½†æ–‡æœ«æœªåˆ—å‡º (Missing) ({len(cited_but_not_listed)})"):
                        if cited_but_not_listed:
                            st.warning("ä»¥ä¸‹ç´¢å¼•éµ (e.g., apa:author:year æˆ– num:1) åœ¨å…§æ–‡è¢«å¼•ç”¨ï¼Œä½†åœ¨æ–‡æœ«åˆ—è¡¨ä¸­æ‰¾ä¸åˆ°ã€‚")
                            for i, key in enumerate(cited_but_not_listed, 1):
                                st.markdown(f"{i}. `{key}`")
                        else:
                            st.success("æ‰€æœ‰å…§æ–‡å¼•ç”¨å‡å°æ‡‰åˆ°æ–‡æœ«åƒè€ƒæ–‡ç»ã€‚")


        # ... (ä¸‹è¼‰çµæœçš„ CSV é‚è¼¯ä¿æŒä¸è®Š) ...
        
        st.markdown("---")

        export_data = []
        for result in st.session_state.query_results:
            filename = result["filename"]
            has_any = False

            if result.get("no_reference_section"):
                export_data.append([filename, "", "æŸ¥ç„¡çµæœï¼šæœªè§£æå‡ºåƒè€ƒæ–‡ç»æ®µè½", ""])
                continue
                
            if not result.get("title_pairs") and not result.get("listed_but_not_cited") and not result.get("cited_but_not_listed"):
                # [!] ä¿®æ­£ç‚ºæ›´ç²¾ç¢ºçš„è¨Šæ¯
                if not result.get("title_pairs") and st.session_state.check_citations:
                    export_data.append([filename, "", "æŸ¥ç„¡çµæœï¼šå·²æ‰¾åˆ°åƒè€ƒæ–‡ç»å€æ®µï¼Œä½†æœªè§£æå‡ºä»»ä½•æ–‡ç»æ¨™é¡Œ (å¼•ç”¨å¯©æ ¸çµæœè«‹è¦‹ä¸‹æ–¹)", ""])
                elif not result.get("title_pairs"):
                     export_data.append([filename, "", "æŸ¥ç„¡çµæœï¼šå·²æ‰¾åˆ°åƒè€ƒæ–‡ç»å€æ®µï¼Œä½†æœªè§£æå‡ºä»»ä½•æ–‡ç»æ¨™é¡Œ", ""])
                # else: ç¹¼çºŒåŸ·è¡Œ

            # API æŸ¥è©¢çµæœ
            for ref, title in result.get("title_pairs", []):
                if ref in result["crossref_doi_hits"]:
                    export_data.append([filename, ref, "Crossref æœ‰ DOI è³‡è¨Š", result["crossref_doi_hits"][ref]])
                elif ref in result["scopus_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰", result["scopus_hits"][ref]])
                elif ref in result["scholar_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰", result["scholar_hits"][ref]])
                elif ref in result["scholar_similar"]:
                    export_data.append([filename, ref, "Google Scholar é¡ä¼¼æ¨™é¡Œ", result["scholar_similar"][ref]])
                elif ref in result.get("scholar_remedial", {}):
                    export_data.append([filename, ref, "Google Scholar è£œæ•‘å‘½ä¸­", result["scholar_remedial"][ref]])
                elif ref in result["not_found"]:
                    scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref)}"
                    export_data.append([filename, ref, "æŸ¥ç„¡çµæœ", scholar_url])

            # [!] å¼•ç”¨å¯©æ ¸çµæœ
            for ref in result.get("listed_but_not_cited", []):
                export_data.append([filename, ref, "å¼•ç”¨ä¸ä¸€è‡´ (æ–‡æœ«åˆ—å‡ºä½†å…§æ–‡æœªå¼•ç”¨)", ""])
            
            for key in result.get("cited_but_not_listed", []):
                export_data.append([filename, key, "å¼•ç”¨ä¸ä¸€è‡´ (å…§æ–‡å¼•ç”¨ä½†æ–‡æœ«æœªåˆ—å‡º)", ""])


        # ... (CSV æ¨™é ­å’Œä¸‹è¼‰æŒ‰éˆ• ... ä¿æŒä¸è®Š) ...
        total_refs = sum(len(r.get("title_pairs", [])) for r in st.session_state.query_results) 
        matched_crossref = sum(len(r.get("crossref_doi_hits", {})) for r in st.session_state.query_results) 
        matched_scopus = sum(len(r.get("scopus_hits", {})) for r in st.session_state.query_results) 
        matched_scholar = sum(len(r.get("scholar_hits", {})) for r in st.session_state.query_results) 
        matched_remedial = sum(len(r.get("scholar_remedial", {})) for r in st.session_state.query_results)
        matched_similar = sum(len(r.get("scholar_similar", {})) for r in st.session_state.query_results)
        matched_notfound = sum(len(r.get("not_found", [])) for r in st.session_state.query_results) 

        # [!] æ–°å¢çµ±è¨ˆ
        total_listed_not_cited = sum(len(r.get("listed_but_not_cited", [])) for r in st.session_state.query_results)
        total_cited_not_listed = sum(len(r.get("cited_but_not_listed", [])) for r in st.session_state.query_results)


        header = StringIO()
        header.write(f"å ±å‘Šç”¢å‡ºæ™‚é–“ï¼š{report_time}\n\n")
        header.write("èªªæ˜ï¼š\n")
        header.write("ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰DOIç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸”ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ã€‚åªé‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚\n")
        header.write("æœ¬ç³»çµ±åªæ˜¯ç‚ºäº†æä¾›åˆæ­¥ç¯©é¸ï¼Œæ¯”å°å¾Œæ‡‰æ¥è‘—é€²è¡Œäººå·¥æ ¸å°ï¼Œä»»ä½•äººéƒ½ä¸æ‡‰è©²ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºä»»ä½•å­¸è¡“å€«CRIåˆ¤æ–·ä¹‹åŸºç¤ã€‚\n\n")

        csv_buffer = StringIO()
        csv_buffer.write(header.getvalue())
        if not export_data:
            df_export = pd.DataFrame([[
                "ï¼ˆç„¡æª”æ¡ˆï¼‰", "", "âš ï¸ æ²’æœ‰å¯åŒ¯å‡ºçš„æŸ¥æ ¸çµæœ", ""
            ]], columns=["æª”æ¡ˆåç¨±", "åŸå§‹åƒè€ƒæ–‡ç»", "æŸ¥æ ¸çµæœ", "é€£çµ"])
        else:
            # [!] ä¿®æ­£ CSV æ¨™é ­
            df_export = pd.DataFrame(export_data, columns=["æª”æ¡ˆåç¨±", "åŸå§‹åƒè€ƒæ–‡ç»/ç´¢å¼•éµ", "æŸ¥æ ¸çµæœ", "é€£çµ"])

        df_export.to_csv(csv_buffer, index=False)

        # çµ±è¨ˆæ‰€æœ‰æª”æ¡ˆçš„ç¸½æ•¸
        total_files = len(st.session_state.query_results)
        
        st.markdown(f"""
        ğŸ“Œ æŸ¥æ ¸çµæœèªªæ˜ï¼šæœ¬æ¬¡å…±è™•ç† **{total_files} ç¯‡è«–æ–‡**ï¼Œç¸½å…±æ“·å– **{total_refs} ç¯‡åƒè€ƒæ–‡ç»**ï¼Œå…¶ä¸­ï¼š

        - {matched_crossref} ç¯‡ç‚ºã€ŒCrossref æœ‰ DOI è³‡è¨Šã€
        - {matched_scopus} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰ã€
        - {matched_scholar} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰ã€
        - {matched_remedial} ç¯‡ç‚ºã€ŒGoogle Scholar è£œæ•‘å‘½ä¸­ã€
        - {matched_similar} ç¯‡ç‚ºã€ŒGoogle Scholar é¡ä¼¼æ¨™é¡Œã€
        - {matched_notfound} ç¯‡ç‚ºã€ŒæŸ¥ç„¡çµæœã€
        """)
        
        if st.session_state.check_citations:
            st.markdown(f"""
            ğŸ“Œ å¼•ç”¨å¯©æ ¸ (Beta) çµæœï¼š
            - {total_listed_not_cited} ç¯‡ç‚ºã€Œæ–‡æœ«åˆ—å‡ºä½†å…§æ–‡æœªå¼•ç”¨ã€
            - {total_cited_not_listed} ç­†ç‚ºã€Œå…§æ–‡å¼•ç”¨ä½†æ–‡æœ«æœªåˆ—å‡ºã€
            """)

        st.markdown("---")
        
        st.subheader("ğŸ“¥ ä¸‹è¼‰æŸ¥è©¢çµæœ")

        st.download_button(
            label="ğŸ“¤ ä¸‹è¼‰çµæœ CSV æª”",
            data=csv_buffer.getvalue().encode('utf-8-sig'),
            file_name="reference_results.csv",
            mime="text/csv"
        )
        st.write("ğŸ” è‹¥è¦é‡æ–°ä¸Šå‚³æª”æ¡ˆï¼Œè«‹æŒ‰ä¸‹éµç›¤ä¸Šçš„ F5 æˆ–é»æ“Šç€è¦½å™¨é‡æ–°æ•´ç†æŒ‰éˆ•")

