# app.py å®Œæ•´åŠŸèƒ½é›²ç«¯ç‰ˆ
import streamlit as st
import pandas as pd
import time
import os
import re
import ast 
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. é›²ç«¯ç’°å¢ƒè‡ªå‹•ä¿®å¾©
# app.py é–‹é ­çš„ä¿®å¾©è£œä¸

def ensure_anystyle_installed():
    # 1. å®šç¾©å¯èƒ½å‡ºç¾çš„è·¯å¾‘ (é‡å° Streamlit Cloud çš„ Linux ç’°å¢ƒ)
    possible_paths = [
        "/home/appuser/.local/share/gem/ruby/3.1.0/bin",
        "/home/adminuser/.local/share/gem/ruby/3.1.0/bin",
        subprocess.getoutput("ruby -e 'print Gem.user_dir'") + "/bin"
    ]
    
    # 2. å°‡é€™äº›è·¯å¾‘åŠ å…¥ç³»çµ±ç’°å¢ƒè®Šæ•¸ PATH
    for p in possible_paths:
        if p not in os.environ["PATH"]:
            os.environ["PATH"] = p + os.pathsep + os.environ["PATH"]

    # 3. æ¸¬è©¦æ˜¯å¦èƒ½åŸ·è¡Œ
    try:
        subprocess.run(["anystyle", "--version"], capture_output=True, check=True)
    except:
        with st.spinner("â˜ï¸ æ­£åœ¨åˆå§‹åŒ–é›²ç«¯ AnyStyle ç’°å¢ƒ..."):
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦å†æ¬¡å®‰è£ (åŠ ä¸Š --user-install ç¢ºä¿æ¬Šé™)
            os.system("gem install anystyle-cli --user-install")
            # å†æ¬¡åˆ·æ–°è·¯å¾‘
            new_path = subprocess.getoutput("ruby -e 'print Gem.user_dir'") + "/bin"
            if new_path not in os.environ["PATH"]:
                os.environ["PATH"] = new_path + os.pathsep + os.environ["PATH"]

ensure_anystyle_installed()

# 2. å°å…¥æ¨¡çµ„
try:
    from modules.parsers import parse_references_with_anystyle
    from modules.local_db import load_csv_data, search_local_database
    from modules.api_clients import (
        get_scopus_key, get_serpapi_key, search_crossref_by_doi,
        search_crossref_by_text, search_scopus_by_title,
        search_scholar_by_title, search_scholar_by_ref_text,
        search_s2_by_title, search_openalex_by_title, check_url_availability
    )
except Exception as e:
    st.error(f"âŒ æ¨¡çµ„åŠ è¼‰å¤±æ•—: {e}")

# 3. é é¢è¨­å®šèˆ‡ç²¾ç¾ UI
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (ç³»çµ±å¢å¼·ç‰ˆ)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2.2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1.5rem; }
    .status-badge { padding: 4px 10px; border-radius: 12px; font-size: 0.85em; font-weight: bold; margin-right: 5px; }
    .ref-box { background-color: #f8f9fa; padding: 12px; border-radius: 8px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    div[data-testid="stMarkdownContainer"] table { width: 100%; border-collapse: collapse; margin-bottom: 10px; }
    div[data-testid="stMarkdownContainer"] td { padding: 8px 5px; border-bottom: 1px solid #f0f0f0; font-size: 0.95em; }
    div[data-testid="stMarkdownContainer"] th { display: none; }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (å…¨åŠŸèƒ½å®Œæ•´ç‰ˆ)</div>', unsafe_allow_html=True)

# Session State
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# 4. è¼”åŠ©å‡½å¼ (äººåæ ¼å¼åŒ–èˆ‡è£œæ•‘)
def format_name_field(data):
    if not data: return None
    try:
        if isinstance(data, str):
            if not (data.startswith('[') or data.startswith('{')): return data
            data = ast.literal_eval(data)
        names_list = []
        items = [data] if isinstance(data, dict) else data
        for item in items:
            if isinstance(item, dict):
                parts = [p for p in [item.get('family'), item.get('given')] if p]
                names_list.append(", ".join(parts))
            else: names_list.append(str(item))
        return "; ".join(names_list)
    except: return str(data)

def refine_parsed_data(parsed_item):
    item = parsed_item.copy()
    raw_text = item.get('text', '').strip()
    
    # ç¢ºä¿æ‰€æœ‰åŸºç¤æ¬„ä½éƒ½æ˜¯å­—ä¸²ï¼Œé¿å… re å ±éŒ¯
    for key in ['doi', 'url', 'title', 'date']:
        val = item.get(key)
        if val and isinstance(val, str):
            item[key] = val.strip(' ,.;)]}>')
        elif val is not None:
            item[key] = str(val) # å¼·åˆ¶è½‰å­—ä¸²

    # æ¨™é¡Œè£œæ•‘é‚è¼¯
    title = item.get('title', '')
    if not title or len(title) < 10:
        abbr_match = re.search(r'^([A-Z0-9\-\.\s]{2,12}:\s*.+?)(?=\s*[,\[]|\s*Available|\s*\(|\bhttps?://|\.|$)', raw_text)
        if abbr_match:
            item['title'] = abbr_match.group(1).strip()
        else:
            for backup_key in ['publisher', 'container-title', 'journal']:
                val = item.get(backup_key)
                if val and len(str(val)) > 15:
                    item['title'] = str(val).strip()
                    break

    # DOI æå–è£œæ•‘ (å®‰å…¨æ€§ä¿®æ­£)
    current_url = item.get('url')
    if current_url and isinstance(current_url, str):
        doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', current_url)
        if doi_match: 
            item['doi'] = doi_match.group(1).strip('.')

    if item.get('authors'): 
        item['authors'] = format_name_field(item['authors'])
    return item

# 5. å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ ç³»çµ±è¨­å®š")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥æœ¬åœ°åº«: {len(local_df)} ç­†")
            target_col = "è«–æ–‡åç¨±" if "è«–æ–‡åç¨±" in local_df.columns else local_df.columns[0]
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"Scopus API: {'âœ…' if scopus_key else 'âŒ'}")
    st.info(f"SerpAPI: {'âœ…' if serpapi_key else 'âŒ'}")

# 6. ä¸»é é¢
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

with tab1:
    raw_input = st.text_area("åœ¨æ­¤è¼¸å…¥æ–‡ç»å…§å®¹...", height=300, placeholder="è²¼ä¸Š References...")
    if st.button("ğŸš€ é–‹å§‹è§£æ", type="primary"):
        if not raw_input: 
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡ç»å…§å®¹")
        else:
            # æ¸…ç©ºèˆŠçµæœ
            st.session_state.structured_references = []
            st.session_state.results = []
            
            with st.spinner("æ­£åœ¨è§£ææ–‡ç»çµæ§‹..."):
                # é€™è£¡æœ€é—œéµï¼šå¿…é ˆæ¥æ”¶å…©å€‹å€¼ (raw_texts, struct_list)
                raw_list, struct_list = parse_references_with_anystyle(raw_input)
                
                if struct_list:
                    st.session_state.structured_references = struct_list
                    st.success(f"âœ… è§£æå®Œæˆï¼Œå…± {len(struct_list)} ç­†ï¼è«‹åˆ‡æ›è‡³ã€Œé©—è­‰çµæœã€é ç±¤ã€‚")
                    # å¼·åˆ¶é é¢æ›´æ–°ï¼Œé€™æ¨£ Tab2 æ‰æœƒçœ‹åˆ°è³‡æ–™
                    time.sleep(1)
                    st.rerun() 
                else:
                    st.error("è§£æå¤±æ•—ï¼Œè«‹ç¢ºèª Log æˆ–è¼¸å…¥æ ¼å¼ã€‚")

with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆè§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹ä¸¦è¡Œé©—è­‰", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_task(idx, raw_ref):
                ref = refine_parsed_data(raw_ref)
                title = ref.get('title', '')
                text = ref.get('text', '')
                search_query = title if (title and len(title) > 8) else text[:120]
                doi = ref.get('doi')
                parsed_url = ref.get('url')
                first_author = ref['authors'].split(';')[0].split(',')[0].strip() if ref.get('authors') else ""

                res = {"id": idx, "title": title, "text": text, "parsed": ref, "sources": {}, "found_at_step": None, "debug_logs": {}, "suggestion": None}

                # Step 0: Local DB
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', search_query))
                if has_chinese and local_df is not None and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref / Scopus ...
                if doi:
                    _, url, _ = search_crossref_by_doi(doi, target_title=title if title else None)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref (DOI)"
                        return res
                
                for api_func, step_name in [
                    (lambda: search_crossref_by_text(search_query, first_author), "1. Crossref"),
                    (lambda: search_scopus_by_title(search_query, scopus_key) if scopus_key else (None, "No Key"), "2. Scopus"),
                    (lambda: search_openalex_by_title(search_query, first_author), "3. OpenAlex"),
                    (lambda: search_s2_by_title(search_query, first_author), "4. Semantic Scholar"),
                    (lambda: search_scholar_by_title(search_query, serpapi_key) if serpapi_key else (None, "No Key"), "5. Google Scholar")
                ]:
                    try:
                        u, status = api_func()
                        if u:
                            res["sources"][step_name.split(". ")[1]] = u
                            res["found_at_step"] = step_name
                            return res
                        res["debug_logs"][step_name] = status
                    except: pass

                if serpapi_key:
                    url_r, _ = search_scholar_by_ref_text(text, serpapi_key, target_title=title)
                    if url_r: res["suggestion"] = url_r

                if parsed_url and parsed_url.startswith('http'):
                    if check_url_availability(parsed_url):
                        res["sources"]["Direct Link"] = parsed_url
                        res["found_at_step"] = "6. Website / Direct URL"
                    else:
                        res["sources"]["Direct Link (Dead)"] = parsed_url
                        res["found_at_step"] = "6. Website (Link Failed)"
                return res

            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(check_single_task, i+1, r): i for i, r in enumerate(refs)}
                for i, future in enumerate(as_completed(futures)):
                    results_buffer.append(future.result())
                    progress.progress((i + 1) / total)
                    status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")
            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            st.rerun()

        # --- å®Œæ•´çµæœå±•ç¤º (ä¿®æ­£ç¸®é€²éŒ¯èª¤å€) ---
        if st.session_state.results:
            # 1. è¨ˆç®—æ•¸æ“š
            total_count = len(st.session_state.results)
            db_count = sum(1 for r in st.session_state.results if r.get('found_at_step') and "Website" not in r.get('found_at_step'))
            web_count = sum(1 for r in st.session_state.results if r.get('found_at_step') == "6. Website / Direct URL")
            fail_count = total_count - db_count - web_count

            # 2. é¡¯ç¤ºçµ±è¨ˆ
            st.markdown("### ğŸ“Š é©—è­‰å³æ™‚çµ±è¨ˆ")
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ç¸½æ–‡ç»æ•¸", total_count)
            c2.metric("âœ… è³‡æ–™åº«æˆåŠŸ", db_count)
            c3.metric("ğŸŒ ç¶²ç«™ä¾†æº", web_count)
            c4.metric("âŒ æœªæ‰¾åˆ°", fail_count, delta="-"+str(fail_count) if fail_count > 0 else None)
            
            st.divider()
            filter_option = st.selectbox("ğŸ“‚ ç¯©é¸é¡¯ç¤ºçµæœ", ["å…¨éƒ¨é¡¯ç¤º", "âœ… è³‡æ–™åº«é©—è­‰", "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº", "âŒ æœªæ‰¾åˆ°çµæœ"])
            
            # 3. å¾ªç’°é¡¯ç¤ºæ–‡ç»çµæœ (æ­¤è™•ç¸®é€²å·²å°é½Š)
            for res in st.session_state.results:
                found_step = res.get('found_at_step')
                is_db = found_step and "Website" not in found_step
                is_web = found_step == "6. Website / Direct URL"
                is_fail = found_step == "6. Website (Link Failed)"

                if filter_option == "âœ… è³‡æ–™åº«é©—è­‰" and not is_db: continue
                if filter_option == "ğŸŒ ç¶²ç«™æœ‰æ•ˆä¾†æº" and not is_web: continue
                if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and (is_db or is_web or is_fail): continue

                bg = "#D1FAE5" if is_db else ("#DBEAFE" if is_web else ("#FEF3C7" if is_fail else "#FEE2E2"))
                label = f"âœ… {found_step}" if is_db else (f"ğŸŒ {found_step}" if is_web else (f"âš ï¸ {found_step}" if is_fail else "âŒ æœªæ‰¾åˆ°"))
                p = res.get('parsed', {})

                with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                    st.markdown(f'<div style="background:{bg}; padding:10px; border-radius:5px; margin-bottom:10px;"><b>ç‹€æ…‹:</b> {label}</div>', unsafe_allow_html=True)
                    
                    st.markdown(f"""
                    | | |
                    | :--- | :--- |
                    | **ğŸ‘¥ ä½œè€…/ç·¨è€…** | `{p.get('authors', 'N/A')}` |
                    | **ğŸ“… ç™¼è¡¨å¹´ä»½** | `{p.get('date', 'N/A')}` |
                    | **ğŸ“° æ–‡ç»æ¨™é¡Œ** | `{p.get('title', 'N/A')}` |
                    | **ğŸ¢ å‡ºè™•/ç™¼è¡Œ** | `{p.get('journal', p.get('publisher', 'N/A'))}` |
                    """)
                    
                    st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                    st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                    
                    if res.get("suggestion"):
                        st.warning(f"ğŸ’¡ [å»ºè­°çµæœ (Google Scholar)]({res['suggestion']})")

                    if res['sources']:
                        st.write("**ğŸ”— é©—è­‰é€£çµï¼š**")
                        for src, link in res['sources'].items():
                            st.write(f"- {src}: [{link}]({link})")
                    else:
                        with st.expander("ğŸ” æŸ¥çœ‹ Debug Logs"):
                            for api, msg in res.get("debug_logs", {}).items():
                                st.write(f"**{api}**: {msg}")

with tab3:
    if st.session_state.results:
        df = pd.DataFrame(st.session_state.results)
        st.bar_chart(df['found_at_step'].value_counts())
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š", df.to_csv(index=False).encode('utf-8-sig'), "report.csv")
    else:
        st.info("å°šç„¡æ•¸æ“š")
