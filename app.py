import streamlit as st
import pandas as pd
import time
import os
import re
import ast
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==============================================================================
# 1. è‡ªå‹•ç’°å¢ƒä¿®å¾©é‚è¼¯ (ä¿ç•™ä½ åŸæœ¬æˆåŠŸçš„è§£æ±ºæ–¹æ¡ˆ)
# ==============================================================================
def ensure_anystyle_installed():
    try:
        # æª¢æŸ¥ anystyle æŒ‡ä»¤æ˜¯å¦å¯ç”¨
        subprocess.run(["ruby", "-S", "anystyle", "--version"], capture_output=True, check=True)
    except:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ– AnyStyle ç’°å¢ƒï¼ˆå®‰è£ Ruby å¥—ä»¶ï¼‰..."):
            # ä½¿ç”¨ --user-install é¿é–‹æ¬Šé™å•é¡Œ
            os.system("gem install anystyle-cli --user-install")
            # å°‡ User gem path åŠ å…¥ç’°å¢ƒè®Šæ•¸
            user_gem_path = subprocess.getoutput("ruby -e 'print Gem.user_dir'") + "/bin"
            os.environ["PATH"] += os.pathsep + user_gem_path

ensure_anystyle_installed()

# ==============================================================================
# 2. å°å…¥æ¨¡çµ„
# ==============================================================================
try:
    from modules.parsers import parse_references_with_anystyle
    from modules.local_db import load_csv_data, search_local_database
    from modules.api_clients import (
        get_scopus_key, get_serpapi_key, search_crossref_by_doi,
        search_crossref_by_text, search_scopus_by_title,
        search_scholar_by_title, search_scholar_by_ref_text,
        search_s2_by_title, search_openalex_by_title, check_url_availability
    )
except ImportError as e:
    st.error(f"âŒ å°å…¥æ¨¡çµ„å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºä¿ modules æª”æ¡ˆå¤¾å®Œæ•´ã€‚")

# ==============================================================================
# 3. é é¢è¨­å®šèˆ‡æ¨£å¼
# ==============================================================================
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (çµ‚æ¥µæ•´åˆç‰ˆ)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2.2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 10px; border-radius: 12px; font-size: 0.85em; font-weight: bold; margin-right: 5px; }
    .ref-box { background-color: #f8f9fa; padding: 12px; border-radius: 8px; font-family: 'Courier New', monospace; font-size: 0.9em; border: 1px solid #e0e0e0; }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (ç³»çµ±æ•´åˆå¢å¼·ç‰ˆ)</div>', unsafe_allow_html=True)

if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ==============================================================================
# 4. æ ¸å¿ƒè¼”åŠ©å·¥å…·ï¼šäººåèˆ‡è³‡æ–™è£œæ•‘
# ==============================================================================
def format_name_field(data):
    if not data: return None
    if isinstance(data, str) and not (data.startswith('[') or data.startswith('{')): return data
    try:
        if isinstance(data, str):
            try: data = ast.literal_eval(data)
            except: return data
        names_list = []
        items = data if isinstance(data, list) else [data]
        for item in items:
            if isinstance(item, dict):
                parts = [p for p in [item.get('family'), item.get('given')] if p]
                names_list.append(", ".join(parts))
            else:
                names_list.append(str(item))
        return "; ".join(names_list)
    except: return str(data)

def refine_parsed_data(parsed_item):
    """ æ•´åˆåŒå­¸çš„é€²éšæ¸…æ´—é‚è¼¯ """
    item = parsed_item.copy()
    raw_text = item.get('text', '').strip()

    # åŸºç¤æ¸…æ´—
    for key in ['doi', 'url', 'title', 'date']:
        if item.get(key) and isinstance(item[key], str):
            item[key] = item[key].strip(' ,.;)]}>')

    # DOI æå–èˆ‡ URL ä¿®å¾©
    url_val = item.get('url', '')
    if url_val:
        doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', url_val)
        if doi_match:
            item['doi'] = doi_match.group(1).strip('.')

    # æ¨™é¡Œè£œæ•‘é‚è¼¯
    title = item.get('title', '')
    if not title or len(title) < 10:
        # å˜—è©¦å¾å…¶ä»–æ¬„ä½æ’ˆå– (AnyStyle å¸¸èª¤åˆ¤æ¨™é¡Œç‚ºæœŸåˆŠæˆ–å‡ºç‰ˆå•†)
        for backup_key in ['publisher', 'container-title', 'journal']:
            val = item.get(backup_key)
            if val and isinstance(val, str) and len(val) > 15:
                item['title'] = val.strip()
                break
    
    # æ ¼å¼åŒ–äººå
    if item.get('authors'): item['authors'] = format_name_field(item['authors'])
    return item

# ==============================================================================
# 5. å´é‚Šæ¬„è¨­å®š
# ==============================================================================
with st.sidebar:
    st.header("âš™ï¸ ç³»çµ±è¨­å®š")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… æœ¬åœ°åº«è¼‰å…¥: {len(local_df)} ç­†")
            target_col = "è«–æ–‡åç¨±" if "è«–æ–‡åç¨±" in local_df.columns else local_df.columns[0]
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"API ç‹€æ…‹:\n- Scopus: {'âœ…' if scopus_key else 'âŒ'}\n- SerpAPI: {'âœ…' if serpapi_key else 'âŒ'}")

# ==============================================================================
# 6. ä¸»åŠŸèƒ½å€
# ==============================================================================
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    raw_input = st.text_area("åœ¨æ­¤è²¼ä¸Šæ–‡ç»å…§å®¹...", height=300, placeholder="ç›´æ¥è²¼ä¸Šè«–æ–‡æœ«å°¾çš„ References åˆ—è¡¨")
    
    if st.button("ğŸš€ é–‹å§‹è§£æ", type="primary"):
        if not raw_input.strip():
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            with st.spinner("AnyStyle å¼•æ“è™•ç†ä¸­..."):
                _, struct_list = parse_references_with_anystyle(raw_input)
            if struct_list:
                st.session_state.structured_references = struct_list
                st.success(f"âœ… è§£ææˆåŠŸï¼å…± {len(struct_list)} ç­†ã€‚")
            else:
                st.error("âŒ AnyStyle è§£æç•°å¸¸ï¼Œè«‹æª¢æŸ¥ Ruby ç’°å¢ƒã€‚")

with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹å…¨è‡ªå‹•ä¸¦è¡Œé©—è­‰", type="primary"):
            st.session_state.results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_task(idx, raw_ref):
                ref = refine_parsed_data(raw_ref)
                title = ref.get('title', '')
                text = ref.get('text', '')
                doi = ref.get('doi')
                parsed_url = ref.get('url')
                first_author = ref['authors'].split(';')[0].split(',')[0].strip() if ref.get('authors') else ""

                res = {
                    "id": idx, "title": title if title else "è§£æå¤±æ•— (ä¿åº•æœå°‹)", 
                    "text": text, "parsed": ref, "sources": {}, "found_at_step": None, 
                    "debug_logs": {}, "suggestion": None
                }

                # æœå°‹åºåˆ—
                # Step 0: Local DB
                if bool(re.search(r'[\u4e00-\u9fff]', title)) and local_df is not None and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref
                if doi:
                    _, url, _ = search_crossref_by_doi(doi, target_title=title)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref (DOI)"
                        return res
                
                url, _ = search_crossref_by_text(title if len(title)>10 else text[:100], first_author)
                if url:
                    res["sources"]["Crossref"] = url
                    res["found_at_step"] = "1. Crossref (Search)"
                    return res

                # Step 2: API åºåˆ— (OpenAlex -> S2 -> Scopus -> Scholar)
                for func, name in [
                    (lambda: search_openalex_by_title(title, first_author), "OpenAlex"),
                    (lambda: search_s2_by_title(title, first_author), "Semantic Scholar"),
                    (lambda: search_scopus_by_title(title, scopus_key) if scopus_key else (None, None), "Scopus"),
                    (lambda: search_scholar_by_title(title, serpapi_key) if serpapi_key else (None, None), "Google Scholar")
                ]:
                    try:
                        u, s = func()
                        if u:
                            res["sources"][name] = u
                            res["found_at_step"] = f"2. {name}"
                            return res
                    except: pass

                # Step 3: ç¶²ç«™æª¢æŸ¥
                if parsed_url and parsed_url.startswith('http'):
                    if check_url_availability(parsed_url):
                        res["sources"]["Direct Link"] = parsed_url
                        res["found_at_step"] = "3. Website / Direct URL"
                        return res

                # è£œæ•‘æ©Ÿåˆ¶ï¼šScholar Ref Text å»ºè­°
                if serpapi_key:
                    url_r, _ = search_scholar_by_ref_text(text, serpapi_key, target_title=title)
                    if url_r: res["suggestion"] = url_r

                return res

            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(check_single_task, i+1, r): i for i, r in enumerate(refs)}
                for i, future in enumerate(as_completed(futures)):
                    results_buffer.append(future.result())
                    progress_bar.progress((i + 1) / total)
                    status_text.text(f"å·²å®Œæˆ: {i+1}/{total}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            st.rerun()

    # å±•ç¤ºèˆ‡ç¯©é¸é‚è¼¯
    if st.session_state.results:
        # (æ­¤è™•å¯åŠ å…¥ä½ åŸæœ¬çš„ç¯©é¸çµ±è¨ˆ Badge ç¨‹å¼ç¢¼ï¼Œç©ºé–“é—œä¿‚çœç•¥ä½†é‚è¼¯ç›¸åŒ)
        for res in st.session_state.results:
            p = res['parsed']
            with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                st.write(f"**ç‹€æ…‹:** {res['found_at_step'] or 'âŒ æœªæ‰¾åˆ°'}")
                st.json(p) # æ–¹ä¾¿ Debug
                if res['sources']:
                    for s, link in res['sources'].items():
                        st.markdown(f"ğŸ”— **[{s}]({link})**")
                if res['suggestion']:
                    st.warning(f"ğŸ’¡ å»ºè­°æŸ¥çœ‹ç›¸ä¼¼æ–‡ç»: [Google Scholar]({res['suggestion']})")

with tab3:
    if st.session_state.results:
        df = pd.DataFrame(st.session_state.results)
        st.bar_chart(df['found_at_step'].fillna('Not Found').value_counts())
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ CSV", df.to_csv(index=False).encode('utf-8-sig'), "report.csv")
    else:
        st.info("å°šç„¡æ•¸æ“š")
