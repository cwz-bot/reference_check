# app.py

import streamlit as st
import pandas as pd
import time
import os
import re
import ast 
from concurrent.futures import ThreadPoolExecutor, as_completed

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from modules.parsers import parse_references_with_anystyle
# å°å…¥æœ¬åœ°è³‡æ–™åº«æ¨¡çµ„
from modules.local_db import load_csv_data, search_local_database
# å°å…¥ API å®¢æˆ¶ç«¯
from modules.api_clients import (
    get_scopus_key,
    get_serpapi_key,
    search_crossref_by_doi,
    search_scopus_by_title,
    search_scholar_by_title,
    search_scholar_by_ref_text,
    search_s2_by_title,
    search_openalex_by_title
)

# ========== é é¢è¨­å®š ==========
st.set_page_config(page_title="å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (Local DB + Docker)", page_icon="ğŸ“š", layout="wide")

st.markdown("""
<style>
    .main-header { font-size: 2rem; font-weight: bold; text-align: center; color: #4F46E5; margin-bottom: 1rem; }
    .status-badge { padding: 4px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
    .ref-box { background-color: #f8f9fa; padding: 10px; border-radius: 5px; font-family: monospace; font-size: 0.9em; color: #333; border: 1px solid #ddd; }
    
    /* è¡¨æ ¼æ¨£å¼å„ªåŒ–ï¼šéš±è—è¡¨é ­ */
    div[data-testid="stMarkdownContainer"] table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 10px;
    }
    div[data-testid="stMarkdownContainer"] td {
        padding: 8px 5px;
        border-bottom: 1px solid #f0f0f0;
        font-size: 0.95em;
    }
    div[data-testid="stMarkdownContainer"] th {
        display: none; /* éš±è—è¡¨é ­ */
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ“š å­¸è¡“å¼•ç”¨æª¢æŸ¥å™¨ (æ··åˆé›²åœ°ç‰ˆ)</div>', unsafe_allow_html=True)

# ========== Session State ==========
if "structured_references" not in st.session_state: st.session_state.structured_references = []
if "results" not in st.session_state: st.session_state.results = []

# ========== [è¼”åŠ©] 1. äººåæ ¼å¼åŒ– ==========
def format_name_field(data):
    """å°‡ AnyStyle å›å‚³çš„è¤‡é›œäººåæ ¼å¼çµ±ä¸€è½‰ç‚ºæ˜“è®€å­—ä¸²ã€‚"""
    if not data: return None
    if isinstance(data, str) and not (data.startswith('[') or data.startswith('{')): return data

    try:
        if isinstance(data, str):
            try: data = ast.literal_eval(data)
            except: return data

        names_list = []
        if isinstance(data, dict): data = [data]
        elif not isinstance(data, list): return str(data)

        for item in data:
            if isinstance(item, dict):
                parts = []
                if item.get('family'): parts.append(item['family'])
                if item.get('given'): parts.append(item['given'])
                if parts: names_list.append(", ".join(parts))
            else:
                names_list.append(str(item))
        return "; ".join(names_list)
    except:
        return str(data)

# ========== [æ ¸å¿ƒä¿®æ”¹] 2. è³‡æ–™æ¸…æ´—èˆ‡æ‹†åˆ†ä¿®æ­£ (Post-Processing) ==========
def refine_parsed_data(parsed_item):
    """
    ä¿®æ­£ AnyStyle è§£æä¸å®Œç¾çš„æ¬„ä½ (ç´”é‚è¼¯ä¿®å¾©)ã€‚
    """
    item = parsed_item.copy()

    # --- [ä¿®æ­£] æ›´å¼·çš„ Regexï¼šè™•ç† "(2nd ed.) Routledge" ---
    # èªªæ˜ï¼š
    # 1. ^([(\[]?.*?(?:ed\.|edition|edn)[)\]]?) -> æŠ“å–é–‹é ­å«æœ‰ ed./edition çš„éƒ¨åˆ† (Group 1)ï¼Œå…è¨±æ‹¬è™Ÿ
    # 2. \s*[:.,]?\s* -> å¿½ç•¥ä¸­é–“çš„ç¬¦è™Ÿ
    # 3. (.+)$ -> å‰©ä¸‹çš„å…¨éƒ¨æŠ“ç‚ºå‡ºç‰ˆç¤¾ (Group 2)
    if item.get('edition') and not item.get('publisher'):
        ed_text = item['edition']
        match = re.search(r'^([(\[]?.*?(?:ed\.|edition|edn)[)\]]?)\s*[:.,]?\s*(.+)$', ed_text, re.IGNORECASE)
        
        if match:
            item['edition'] = match.group(1).strip()       # ä¾‹å¦‚: (2nd ed.)
            item['publisher'] = match.group(2).strip(' .,') # ä¾‹å¦‚: Routledge
    
    # --- æ ¼å¼åŒ–äººå ---
    if item.get('authors'): item['authors'] = format_name_field(item['authors'])
    if item.get('editor'): item['editor'] = format_name_field(item['editor'])

    return item

# ========== å´é‚Šæ¬„ (ä¿æŒä¸è®Š) ==========
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    st.subheader("ğŸ“‚ æœ¬åœ°è³‡æ–™åº« (å„ªå…ˆæª¢æŸ¥)")
    DEFAULT_CSV_PATH = "112ndltd.csv"
    local_df = None
    target_col = None
    
    if os.path.exists(DEFAULT_CSV_PATH):
        @st.cache_data
        def read_data_cached(file): return load_csv_data(file)
        local_df = read_data_cached(DEFAULT_CSV_PATH)
        if local_df is not None:
            st.success(f"âœ… å·²è¼‰å…¥å…§å»ºè³‡æ–™åº«: {len(local_df)} ç­†è³‡æ–™")
            default_idx = 0
            if "è«–æ–‡åç¨±" in local_df.columns: default_idx = list(local_df.columns).index("è«–æ–‡åç¨±")
            target_col = st.selectbox("æ¯”å°æ¬„ä½:", options=local_df.columns, index=default_idx, disabled=True)
            st.info("ğŸ’¡ ç³»çµ±å„ªå…ˆæœå°‹æœ¬åœ°åº« (é™ä¸­æ–‡æ–‡ç»)ï¼Œæ‰¾ä¸åˆ°æ‰è¯ç¶²ã€‚")
    else:
        st.error(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é è¨­æª”æ¡ˆ {DEFAULT_CSV_PATH}")
    
    st.divider()
    scopus_key = get_scopus_key()
    serpapi_key = get_serpapi_key()
    st.info(f"Scopus API: {'âœ… å·²è¼‰å…¥' if scopus_key else 'âŒ æœªè¨­å®š'}")
    st.info(f"SerpAPI: {'âœ… å·²è¼‰å…¥' if serpapi_key else 'âŒ æœªè¨­å®š'}")
    st.divider()
    st.subheader("ğŸ” æª¢æŸ¥é †åº")
    st.markdown("""
    1. **æœ¬åœ° CSV è³‡æ–™åº«** (åƒ…é™ä¸­æ–‡)
    2. **Crossref** (DOI)
    3. **Scopus**
    4. **OpenAlex**
    5. **Semantic Scholar**
    6. **Google Scholar**
    """)
    check_crossref = True
    check_scopus = True
    check_openalex = True
    check_s2 = True
    check_scholar = True

# ========== ä¸»é‚è¼¯ ==========
tab1, tab2, tab3 = st.tabs(["ğŸ“ è¼¸å…¥èˆ‡è§£æ", "ğŸ” é©—è­‰çµæœ", "ğŸ“Š çµ±è¨ˆå ±å‘Š"])

# --- TAB 1: è¼¸å…¥ ---
with tab1:
    st.subheader("è²¼ä¸Šåƒè€ƒæ–‡ç»åˆ—è¡¨")
    st.info("ğŸ’¡ è«‹ç›´æ¥è²¼ä¸Šæ•´æ®µæ–‡ç»ï¼ŒDocker å®¹å™¨å…§çš„ AnyStyle æœƒè‡ªå‹•è­˜åˆ¥ä¸¦æ‹†åˆ†ã€‚")
    raw_input = st.text_area("åœ¨æ­¤è²¼ä¸Šå…§å®¹...", height=300)
    
    if st.button("ğŸš€ ä½¿ç”¨ AnyStyle è§£æ", type="primary"):
        if not raw_input:
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—")
        else:
            st.session_state.structured_references = []
            st.session_state.results = []
            
            with st.spinner("æ­£åœ¨å‘¼å« Docker å®¹å™¨é€²è¡Œè§£æ..."):
                raw_list, struct_list = parse_references_with_anystyle(raw_input)
            
            if struct_list:
                st.session_state.structured_references = struct_list
                st.success(f"âœ… è§£ææˆåŠŸï¼å…±è­˜åˆ¥å‡º {len(struct_list)} ç­†æ–‡ç»ã€‚")
                with st.expander("ğŸ” é è¦½è§£æçµæœ (Debug JSON)"):
                    st.json(struct_list[:3])
            else:
                st.error("è§£æå¤±æ•—ï¼Œè«‹ç¢ºèª Docker æ˜¯å¦æ­£åœ¨åŸ·è¡Œã€‚")

# --- TAB 2: æª¢æŸ¥ ---
with tab2:
    if not st.session_state.structured_references:
        st.info("è«‹å…ˆåœ¨ç¬¬ä¸€é è¼¸å…¥ä¸¦è§£ææ–‡ç»ã€‚")
    else:
        if st.button("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰æ–‡ç» (å¾ªåºæ¨¡å¼)", type="primary"):
            st.session_state.results = []
            progress = st.progress(0)
            status_text = st.empty()
            
            refs = st.session_state.structured_references
            total = len(refs)
            results_buffer = []

            def check_single_sequential(idx, raw_ref):
                # 1. å…ˆä¿®æ­£ AnyStyle çš„è³‡æ–™
                ref = refine_parsed_data(raw_ref)
                
                title = ref.get('title', '')
                text = ref.get('text', '')
                doi = ref.get('doi')
                
                res = {
                    "id": idx,
                    "title": title,
                    "text": text,
                    "parsed": ref,
                    "sources": {},
                    "found_at_step": None
                }
                
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', title)) if title else False

                # ğŸ›‘ Step 0: Local DB
                if has_chinese and local_df is not None and target_col and title:
                    match_row, score = search_local_database(local_df, target_col, title, threshold=0.85)
                    if match_row is not None:
                        res["sources"]["Local DB"] = "æœ¬åœ°è³‡æ–™åº«åŒ¹é…æˆåŠŸ"
                        res["found_at_step"] = "0. Local Database"
                        return res

                # Step 1: Crossref
                if check_crossref and doi:
                    _, url = search_crossref_by_doi(doi)
                    if url:
                        res["sources"]["Crossref"] = url
                        res["found_at_step"] = "1. Crossref"
                        return res 

                # Step 2: Scopus
                if check_scopus and scopus_key and title:
                    url = search_scopus_by_title(title, scopus_key)
                    if url:
                        res["sources"]["Scopus"] = url
                        res["found_at_step"] = "2. Scopus"
                        return res 

                # Step 3: OpenAlex
                if check_openalex and title:
                    url = search_openalex_by_title(title)
                    if url:
                        res["sources"]["OpenAlex"] = url
                        res["found_at_step"] = "3. OpenAlex"
                        return res 

                # Step 4: Semantic Scholar
                if check_s2 and title:
                    url = search_s2_by_title(title)
                    if url:
                        res["sources"]["Semantic Scholar"] = url
                        res["found_at_step"] = "4. Semantic Scholar"
                        return res 

                # Step 5: Google Scholar
                if check_scholar and serpapi_key:
                    if title:
                        url, status = search_scholar_by_title(title, serpapi_key)
                        if status in ["match", "similar"]:
                            res["sources"]["Google Scholar"] = url
                            res["found_at_step"] = "5. Scholar (Title)"
                            return res 
                    
                    url_r, status_r = search_scholar_by_ref_text(text, serpapi_key)
                    if status_r != "no_result":
                        res["sources"]["Google Scholar (è£œæ•‘)"] = url_r
                        res["found_at_step"] = "5. Scholar (Text)"
                        return res 

                return res

            max_workers = min(5, total)
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(check_single_sequential, i+1, r): i for i, r in enumerate(refs)}
                
                for i, future in enumerate(as_completed(futures)):
                    try:
                        data = future.result()
                        results_buffer.append(data)
                        progress.progress((i + 1) / total)
                        status_text.text(f"æ­£åœ¨æª¢æŸ¥: {i+1}/{total}")
                    except Exception as e:
                        st.error(f"Error on item {i}: {e}")

            st.session_state.results = sorted(results_buffer, key=lambda x: x['id'])
            status_text.success("âœ… é©—è­‰å®Œæˆï¼")
            time.sleep(1)
            st.rerun()

        # ======================================================
        # é¡¯ç¤ºçµæœ
        # ======================================================
        if st.session_state.results:
            st.divider()
            
            col1, col2 = st.columns([1, 3])
            with col1:
                filter_option = st.selectbox(
                    "ğŸ“‚ ç¯©é¸é¡¯ç¤ºçµæœ",
                    ["å…¨éƒ¨é¡¯ç¤º", "âœ… å·²é©—è­‰æˆåŠŸ", "âŒ æœªæ‰¾åˆ°çµæœ"],
                    index=0
                )
            
            verified_count = sum(1 for r in st.session_state.results if r.get('found_at_step'))
            unverified_count = len(st.session_state.results) - verified_count
            with col2:
                st.caption(f"ç¸½è¨ˆ: {len(st.session_state.results)} | âœ… å·²é©—è­‰: {verified_count} | âŒ æœªæ‰¾åˆ°: {unverified_count}")

            st.divider()

            for res in st.session_state.results:
                found_step = res.get('found_at_step')
                is_verified = found_step is not None
                
                if filter_option == "âœ… å·²é©—è­‰æˆåŠŸ" and not is_verified: continue
                if filter_option == "âŒ æœªæ‰¾åˆ°çµæœ" and is_verified: continue

                status_label = f"âœ… {found_step}" if found_step else "âŒ æœªæ‰¾åˆ°"
                bg_color = "#D1FAE5" if found_step else "#FEE2E2"
                
                p = res.get('parsed', {})

                with st.expander(f"{res['id']}. {p.get('title', 'ç„¡æ¨™é¡Œ')[:80]}..."):
                    st.markdown(f"""
                    <div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-bottom: 15px;">
                        <b>ç‹€æ…‹:</b> {status_label}
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # -----------------------------------------------------------------
                    # [é¡¯ç¤ºé‚è¼¯ä¿®æ­£]
                    # -----------------------------------------------------------------
                    
                    # 1. ä½œè€…/ç·¨è€…
                    display_author = p.get('authors')
                    if not display_author and p.get('editor'):
                        display_author = f"{p['editor']} (Ed.)"
                    if not display_author: display_author = "N/A"

                    # 2. æ¨™é¡Œ + ç‰ˆæ¬¡ (å°‡ç‰ˆæ¬¡æ¬åˆ°é€™è£¡é¡¯ç¤º)
                    display_title = p.get('title', 'N/A')
                    if p.get('edition'):
                        # é¡¯ç¤ºæ ¼å¼: Title (2nd ed.)
                        display_title += f" {p['edition']}"

                    # 3. å‡ºè™• (Source) - ç¾åœ¨åªè² è²¬é¡¯ç¤º æœŸåˆŠ/å‡ºç‰ˆç¤¾/ç¶²å€
                    source_parts = []
                    
                    # (A) æœŸåˆŠå
                    if p.get('container-title'): source_parts.append(p['container-title'])
                    elif p.get('journal'): source_parts.append(p['journal'])
                    
                    # (B) å‡ºç‰ˆç¤¾ (ç¶“é refineï¼ŒRoutledge æ‡‰è©²è¢«æ•‘å‡ºä¾†äº†)
                    if p.get('publisher'):
                        pub_str = p['publisher']
                        if p.get('location'): pub_str = f"{p['location']}: {pub_str}"
                        source_parts.append(pub_str)
                    
                    # (C) [é—œéµ] ç‰ˆæ¬¡å·²ç¶“æ¬åˆ° Title äº†ï¼Œé€™è£¡ä¸éœ€è¦å†é¡¯ç¤ºç‰ˆæ¬¡
                    # é€™æ¨£ "å‡ºè™•" æ¬„ä½å°±ä¸æœƒå‡ºç¾å¥‡æ€ªçš„ "2nd"
                    
                    # (D) Note/Genre/URL
                    if p.get('genre'): source_parts.append(p['genre'])
                    if p.get('note'): source_parts.append(p['note'])
                    
                    if not source_parts and p.get('url'): 
                        source_parts.append("Web Source")

                    display_source = ", ".join(source_parts) if source_parts else "N/A"
                    # -----------------------------------------------------------------
                    
                    st.markdown(f"""
                    | | |
                    | :--- | :--- |
                    | **ğŸ‘¥ ä½œè€…/ç·¨è€…** | `{display_author}` |
                    | **ğŸ“… ç™¼è¡¨å¹´ä»½** | `{p.get('date', 'N/A')}` |
                    | **ğŸ“° æ–‡ç»æ¨™é¡Œ** | `{display_title}` |
                    | **ğŸ¢ å‡ºè™•/ç™¼è¡Œ** | `{display_source}` |
                    | **ğŸ”¢ DOI/URL** | `{p.get('doi', p.get('url', 'N/A'))}` |
                    """)
                    
                    st.divider()

                    st.markdown("**ğŸ“œ åŸå§‹æ–‡ç»:**")
                    st.markdown(f"<div class='ref-box'>{res['text']}</div>", unsafe_allow_html=True)
                    
                    if res['sources']:
                        st.write("**ğŸ”— é©—è­‰ä¾†æºé€£çµï¼š**")
                        for src, link in res['sources'].items():
                            if link.startswith("http"):
                                st.markdown(f"- **{src}**: [é»æ“Šé–‹å•Ÿ]({link})")
                            else:
                                st.markdown(f"- **{src}**: {link}")
                    else:
                        st.warning("åœ¨æ‰€æœ‰å•Ÿç”¨çš„è³‡æ–™åº«ä¸­çš†æœªæ‰¾åˆ°åŒ¹é…é …ã€‚")

# --- TAB 3: çµ±è¨ˆ (ä¿æŒä¸è®Š) ---
with tab3:
    if st.session_state.results:
        df = pd.DataFrame(st.session_state.results)
        df['Source'] = df['found_at_step'].fillna('Not Found')
        total = len(df)
        verified_count = len(df[df['Source'] != 'Not Found'])
        col1, col2 = st.columns(2)
        col1.metric("ç¸½æ–‡ç»æ•¸", total)
        col2.metric("æˆåŠŸé©—è­‰æ•¸", verified_count, f"{verified_count/total*100:.1f}%")
        st.subheader("é©—è­‰ä¾†æºåˆ†ä½ˆ")
        st.bar_chart(df['Source'].value_counts())
        
        st.subheader("è©³ç´°è³‡æ–™è¡¨")
        export_data = []
        for r in st.session_state.results:
            row = r['parsed'].copy()
            row['id'] = r['id']
            row['verified_source'] = r.get('found_at_step', 'Not Found')
            row['verified_url'] = list(r['sources'].values())[0] if r['sources'] else ''
            export_data.append(row)
        st.dataframe(pd.DataFrame(export_data), use_container_width=True)
        csv = pd.DataFrame(export_data).to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š CSV", csv, "report.csv", "text/csv")
    else:
        st.info("å°šç„¡æ•¸æ“š")